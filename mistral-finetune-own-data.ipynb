{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFCx6jZU3m11"
   },
   "source": [
    "![title](securly-banner2.jpg)\n",
    "\n",
    "# Mistral for Discern - Fine Tune POC\n",
    "\n",
    "This notebook was adapted from this video: https://youtu.be/kmkcNVvEz-k?si=Ogt1wRFNqYI6zXfw&t=1\n",
    "\n",
    "We will use QLoRA, a fine-tuning method that combines quantization and LoRA. For more information about what those are and how they work, see [this post](https://brev.dev/blog/how-qlora-works).\n",
    "\n",
    "In this notebook, we will load the large model in 4bit using `bitsandbytes` and use LoRA to train using the PEFT library from Hugging Face ðŸ¤—."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "Install libraries, load environment, etc.   You're going to need a .env file that looks like this:\n",
    "\n",
    "```bash\n",
    "ES_KEY_ID=\"HljH...\"\r\n",
    "ES_KEY=\"J2eGSPU...g\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!conda install -qy scikit-learn scipy matplotlib\n",
    "!pip install -q -U python-dotenv\n",
    "!pip install -q -U  elasticsearch\n",
    "!pip install -q -U  datasets # The version in conda is broken?\n",
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load keys and such from the .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VC-9m2yv3m18"
   },
   "source": [
    "## Preparing data \n",
    "\n",
    "To prepare your dataset for loading, we need two `.jsonl` files structured something like this:\n",
    "```json\n",
    "{\"document\": \"journal-entry-for-model-to-predict\"}\n",
    "{\"document\": \"journal-entry-for-model-to-predict-1\"}\n",
    "{\"document\": \"journal-entry-for-model-to-predict-2\"}\n",
    "```\n",
    "\n",
    "You probably only need to do this once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Data from Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Variables</th>\n",
       "      <th>Note</th>\n",
       "      <th>AiTrainingPrompt</th>\n",
       "      <th>ExampleAiOutput</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[alexs, slope formula, aslope]</td>\n",
       "      <td>{'sports-and-athletics': 0.0, 'environmentalis...</td>\n",
       "      <td>The student searched for 'slope formula' which...</td>\n",
       "      <td>You are a Student Web Activity Analyzer develo...</td>\n",
       "      <td>{\\r\\n  \"sports-and-athletics\": \"1\",\\r\\n  \"spor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[adj, hellosmart]</td>\n",
       "      <td>{'sports-and-athletics': 0.0, 'environmentalis...</td>\n",
       "      <td>The search term 'adj' does not provide enough ...</td>\n",
       "      <td>You are a Student Web Activity Analyzer develo...</td>\n",
       "      <td>{\\r\\n  \"sports-and-athletics\": \"1\",\\r\\n  \"spor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[carrie fanart tawog, paswg art style, carrie ...</td>\n",
       "      <td>{'sports-and-athletics': 0.0, 'environmentalis...</td>\n",
       "      <td>The student's web activity shows a strong inte...</td>\n",
       "      <td>You are a Student Web Activity Analyzer develo...</td>\n",
       "      <td>{\\r\\n  \"sports-and-athletics\": \"1\",\\r\\n  \"spor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[how were marriages arranged in ancient babylo...</td>\n",
       "      <td>{'sports-and-athletics': 0.0, 'environmentalis...</td>\n",
       "      <td>The student's web activity indicates a strong ...</td>\n",
       "      <td>You are a Student Web Activity Analyzer develo...</td>\n",
       "      <td>{\\r\\n  \"sports-and-athletics\": \"1\",\\r\\n  \"spor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[kahoot join, lamborghini egoista  black, calc...</td>\n",
       "      <td>{'sports-and-athletics': 0.0, 'environmentalis...</td>\n",
       "      <td>The student's online activities include multip...</td>\n",
       "      <td>You are a Student Web Activity Analyzer develo...</td>\n",
       "      <td>{\\r\\n  \"sports-and-athletics\": \"1\",\\r\\n  \"spor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>[170 140, emma_bydlon, emma bydlon, 7x454, 12x...</td>\n",
       "      <td>{'sports-and-athletics': 1.0, 'environmentalis...</td>\n",
       "      <td>The student showed a strong interest in sports...</td>\n",
       "      <td>You are a Student Web Activity Analyzer develo...</td>\n",
       "      <td>{\\r\\n  \"sports-and-athletics\": \"1\",\\r\\n  \"spor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>[owens in cursive, owens in cowboys number 81 ...</td>\n",
       "      <td>{'sports-and-athletics': 1.0, 'environmentalis...</td>\n",
       "      <td>The student's web activity is heavily focused ...</td>\n",
       "      <td>You are a Student Web Activity Analyzer develo...</td>\n",
       "      <td>{\\r\\n  \"sports-and-athletics\": \"1\",\\r\\n  \"spor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>[suprem hoodie, derhan harman football camp 20...</td>\n",
       "      <td>{'sports-and-athletics': 1.0, 'environmentalis...</td>\n",
       "      <td>The student's web activity shows a strong inte...</td>\n",
       "      <td>You are a Student Web Activity Analyzer develo...</td>\n",
       "      <td>{\\r\\n  \"sports-and-athletics\": \"1\",\\r\\n  \"spor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>[99.math, hello kitty, kuromi, kuromi thankagi...</td>\n",
       "      <td>{'sports-and-athletics': 1.0, 'environmentalis...</td>\n",
       "      <td>The student showed interest in gaming, specifi...</td>\n",
       "      <td>You are a Student Web Activity Analyzer develo...</td>\n",
       "      <td>{\\r\\n  \"sports-and-athletics\": \"1\",\\r\\n  \"spor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>[b-shoc songs, google doodle baseball, how do ...</td>\n",
       "      <td>{'sports-and-athletics': 1.0, 'environmentalis...</td>\n",
       "      <td>The student showed interest in sports through ...</td>\n",
       "      <td>You are a Student Web Activity Analyzer develo...</td>\n",
       "      <td>{\\r\\n  \"sports-and-athletics\": \"1\",\\r\\n  \"spor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Keywords  \\\n",
       "0                       [alexs, slope formula, aslope]   \n",
       "1                                    [adj, hellosmart]   \n",
       "2    [carrie fanart tawog, paswg art style, carrie ...   \n",
       "3    [how were marriages arranged in ancient babylo...   \n",
       "4    [kahoot join, lamborghini egoista  black, calc...   \n",
       "..                                                 ...   \n",
       "495  [170 140, emma_bydlon, emma bydlon, 7x454, 12x...   \n",
       "496  [owens in cursive, owens in cowboys number 81 ...   \n",
       "497  [suprem hoodie, derhan harman football camp 20...   \n",
       "498  [99.math, hello kitty, kuromi, kuromi thankagi...   \n",
       "499  [b-shoc songs, google doodle baseball, how do ...   \n",
       "\n",
       "                                             Variables  \\\n",
       "0    {'sports-and-athletics': 0.0, 'environmentalis...   \n",
       "1    {'sports-and-athletics': 0.0, 'environmentalis...   \n",
       "2    {'sports-and-athletics': 0.0, 'environmentalis...   \n",
       "3    {'sports-and-athletics': 0.0, 'environmentalis...   \n",
       "4    {'sports-and-athletics': 0.0, 'environmentalis...   \n",
       "..                                                 ...   \n",
       "495  {'sports-and-athletics': 1.0, 'environmentalis...   \n",
       "496  {'sports-and-athletics': 1.0, 'environmentalis...   \n",
       "497  {'sports-and-athletics': 1.0, 'environmentalis...   \n",
       "498  {'sports-and-athletics': 1.0, 'environmentalis...   \n",
       "499  {'sports-and-athletics': 1.0, 'environmentalis...   \n",
       "\n",
       "                                                  Note  \\\n",
       "0    The student searched for 'slope formula' which...   \n",
       "1    The search term 'adj' does not provide enough ...   \n",
       "2    The student's web activity shows a strong inte...   \n",
       "3    The student's web activity indicates a strong ...   \n",
       "4    The student's online activities include multip...   \n",
       "..                                                 ...   \n",
       "495  The student showed a strong interest in sports...   \n",
       "496  The student's web activity is heavily focused ...   \n",
       "497  The student's web activity shows a strong inte...   \n",
       "498  The student showed interest in gaming, specifi...   \n",
       "499  The student showed interest in sports through ...   \n",
       "\n",
       "                                      AiTrainingPrompt  \\\n",
       "0    You are a Student Web Activity Analyzer develo...   \n",
       "1    You are a Student Web Activity Analyzer develo...   \n",
       "2    You are a Student Web Activity Analyzer develo...   \n",
       "3    You are a Student Web Activity Analyzer develo...   \n",
       "4    You are a Student Web Activity Analyzer develo...   \n",
       "..                                                 ...   \n",
       "495  You are a Student Web Activity Analyzer develo...   \n",
       "496  You are a Student Web Activity Analyzer develo...   \n",
       "497  You are a Student Web Activity Analyzer develo...   \n",
       "498  You are a Student Web Activity Analyzer develo...   \n",
       "499  You are a Student Web Activity Analyzer develo...   \n",
       "\n",
       "                                       ExampleAiOutput  \n",
       "0    {\\r\\n  \"sports-and-athletics\": \"1\",\\r\\n  \"spor...  \n",
       "1    {\\r\\n  \"sports-and-athletics\": \"1\",\\r\\n  \"spor...  \n",
       "2    {\\r\\n  \"sports-and-athletics\": \"1\",\\r\\n  \"spor...  \n",
       "3    {\\r\\n  \"sports-and-athletics\": \"1\",\\r\\n  \"spor...  \n",
       "4    {\\r\\n  \"sports-and-athletics\": \"1\",\\r\\n  \"spor...  \n",
       "..                                                 ...  \n",
       "495  {\\r\\n  \"sports-and-athletics\": \"1\",\\r\\n  \"spor...  \n",
       "496  {\\r\\n  \"sports-and-athletics\": \"1\",\\r\\n  \"spor...  \n",
       "497  {\\r\\n  \"sports-and-athletics\": \"1\",\\r\\n  \"spor...  \n",
       "498  {\\r\\n  \"sports-and-athletics\": \"1\",\\r\\n  \"spor...  \n",
       "499  {\\r\\n  \"sports-and-athletics\": \"1\",\\r\\n  \"spor...  \n",
       "\n",
       "[500 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "import pandas as pd\n",
    "\n",
    "es = Elasticsearch(\n",
    "    'https://elastic-ksi-prod.es.us-east-1.aws.found.io:443',\n",
    "    api_key=(os.getenv(\"ES_KEY_ID\"), os.getenv(\"ES_KEY\")),                  # Use this if you have an API key\n",
    ")\n",
    "\n",
    "# Define and Execute Query\n",
    "query = {\n",
    "    \"_source\": [\"AiTrainingPrompt\", \"Keywords\", \"ExampleAiOutput\", \"Variables\", \"Note\"],\n",
    "    \"query\": {\n",
    "        \"bool\": {\n",
    "            \"must\": [\n",
    "                {\"match\": {\"ClinicalFrameworkName\": \"Students' Interests - Alpha\"}},\n",
    "                {\"match\": {\"IsSuccess\": True}}\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "response = es.search(index=\"prod-gpt-response\", body=query, size=500)\n",
    "\n",
    "# Process and Display Data\n",
    "hits = response['hits']['hits']\n",
    "data = pd.DataFrame([hit['_source'] for hit in hits])\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting the data back into a single message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Question:\n",
      "You are a Student Web Activity Analyzer developed to support professionals, including Social Workers, School Psychologists, District Administrators, School Safety Specialists, and related roles. Your primary objective is to meticulously evaluate the online activity of K-12 students and identify specific indicators related to their interests and passions. For each identified indicator, provide a JSON object containing:\n",
      "\n",
      "Presence: Indicate a value of 1 (if the indicator is present) or 0 (if not). Even if only a portion of the data aligns with an indicator, mark it as 1.\n",
      "Confidence: Assign a confidence level on a scale of 1-10 to indicate the certainty level of your analysis.\n",
      "\n",
      "Additionally, please include a note that outlines the rationale behind identifying certain indicators and offers a summary of the analyzed web activity.\n",
      "\n",
      "Adhere to the JSON format outlined in the example output section precisely.\n",
      "\n",
      "Each individual online activity you receive represents one search or interaction on a student's device. Occasionally, searches that include large amounts of text will be summarized. These summaries will be marked with 'S~'. Such summarization typically occurs when students copy and paste extensive text blocks, although other cases may exist. Additional details will be provided in the summary.\n",
      "\n",
      "In situations where the presence of an indicator is ambiguous or if anomalies are present in the data, exercise your best judgment while providing a confidence level that reflects the level of uncertainty.\n",
      "\n",
      "Here are the specific indicators that you should use for this task, with definitions, delimited by single quotes.\n",
      "\n",
      "'sports-and-athletics: participating in physical activities and team sports to promote fitness, teamwork, and sportsmanship.'\n",
      "'environmentalism-and-sustainability: learning about the environment, conservation, and sustainable practices to become responsible global citizens.'\n",
      "'gaming-and-e-sports: engaging in digital gaming and competitive e-sports to develop strategic thinking, problem-solving, and teamwork skills.'\n",
      "'college-and-career: engaging in planning, research, and/or discovery around future college and career opportunities or otherwise demonstrating an interest in college or career activities after high school'\n",
      "'cooking-and-food: investigating cooking or food'\n",
      "'reading-and-literature: exploring the world of books and stories through reading and interpretation.'\n",
      "'writing-and-creative-writing: expressing thoughts, ideas, and imagination through written words and storytelling.'\n",
      "'science-and-technology: investigating the natural world and technological advancements'\n",
      "'mathematics-and-statistics: engaging in problem-solving and numerical analysis to understand patterns, shapes, and quantities.'\n",
      "'history-and-social-studies: discovering past events, cultures, and societies to gain a deeper understanding of the world.'\n",
      "'creative-arts: expressing creativity through various art forms like drawing, painting, sculpture, music, performing arts, and more'\n",
      "'animals-and-nature: reflects a student's enthusiasm and curiosity for studying, observing, or interacting with animals and natural environments, potentially driving academic pursuits, extracurricular activities, or career paths related to biology, ecology, or conservation.'\n",
      "\n",
      "\n",
      "### Search Data:\n",
      "['alexs', 'slope formula', 'aslope']\n",
      "\n",
      "### Example Output:\n",
      "{\n",
      "  \"sports-and-athletics\": \"1\",\n",
      "  \"sports-and-athletics-confidence\": \"6\",\n",
      "  \"environmentalism-and-sustainability\": \"0\",\n",
      "  \"environmentalism-and-sustainability-confidence\": \"8\",\n",
      "  \"gaming-and-e-sports\": \"0\",\n",
      "  \"gaming-and-e-sports-confidence\": \"10\",\n",
      "  \"college-and-career\": \"1\",\n",
      "  \"college-and-career-confidence\": \"7\",\n",
      "  \"cooking-and-food\": \"1\",\n",
      "  \"cooking-and-food-confidence\": \"7\",\n",
      "  \"reading-and-literature\": \"0\",\n",
      "  \"reading-and-literature-confidence\": \"7\",\n",
      "  \"writing-and-creative-writing\": \"1\",\n",
      "  \"writing-and-creative-writing-confidence\": \"8\",\n",
      "  \"science-and-technology\": \"0\",\n",
      "  \"science-and-technology-confidence\": \"10\",\n",
      "  \"mathematics-and-statistics\": \"1\",\n",
      "  \"mathematics-and-statistics-confidence\": \"6\",\n",
      "  \"history-and-social-studies\": \"9\",\n",
      "  \"history-and-social-studies-confidence\": \"0\",\n",
      "  \"creative-arts\": \"1\",\n",
      "  \"creative-arts-confidence\": \"8\",\n",
      "  \"animals-and-nature\": \"1\",\n",
      "  \"animals-and-nature-confidence\": \"9\",\n",
      "  \"note\": \"Detailed Summary Goes Here \"\n",
      "}\n",
      "\n",
      "### Solution:\n",
      "{\n",
      "  \"sports-and-athletics\": \"0.0\",\n",
      "  \"sports-and-athletics-confidence\": \"10.0\",\n",
      "  \"environmentalism-and-sustainability\": \"0.0\",\n",
      "  \"environmentalism-and-sustainability-confidence\": \"10.0\",\n",
      "  \"gaming-and-e-sports\": \"0.0\",\n",
      "  \"gaming-and-e-sports-confidence\": \"10.0\",\n",
      "  \"college-and-career\": \"0.0\",\n",
      "  \"college-and-career-confidence\": \"10.0\",\n",
      "  \"cooking-and-food\": \"0.0\",\n",
      "  \"cooking-and-food-confidence\": \"10.0\",\n",
      "  \"reading-and-literature\": \"0.0\",\n",
      "  \"reading-and-literature-confidence\": \"10.0\",\n",
      "  \"writing-and-creative-writing\": \"0.0\",\n",
      "  \"writing-and-creative-writing-confidence\": \"10.0\",\n",
      "  \"science-and-technology\": \"0.0\",\n",
      "  \"science-and-technology-confidence\": \"10.0\",\n",
      "  \"mathematics-and-statistics\": \"1.0\",\n",
      "  \"mathematics-and-statistics-confidence\": \"9.0\",\n",
      "  \"history-and-social-studies\": \"0.0\",\n",
      "  \"history-and-social-studies-confidence\": \"10.0\",\n",
      "  \"creative-arts\": \"0.0\",\n",
      "  \"creative-arts-confidence\": \"10.0\",\n",
      "  \"animals-and-nature\": \"0.0\",\n",
      "  \"animals-and-nature-confidence\": \"10.0\",\n",
      "  \"note\": \"The student searched for 'slope formula' which is related to mathematics, indicating an interest or homework activity in this subject. The confidence is high due to the specific nature of the search term. The other search term 'alexs' is ambiguous and does not provide enough context to confidently associate with any of the given indicators.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def question_func(data):\n",
    "    question = \"\"\"### Question:\n",
    "{prompt}\n",
    "\n",
    "### Search Data:\n",
    "{searches}\n",
    "\n",
    "### Example Output:\n",
    "{example}\n",
    "\n",
    "### Solution:\"\"\".format(prompt=data['AiTrainingPrompt'], searches=data['Keywords'], example=data['ExampleAiOutput'])\n",
    "\n",
    "    return question\n",
    "\n",
    "def solution_func(data):\n",
    "    solution = \"\"\"\n",
    "{{\n",
    "  \"sports-and-athletics\": \"{sportsandathletics}\",\n",
    "  \"sports-and-athletics-confidence\": \"{sportsandathleticsconfidence}\",\n",
    "  \"environmentalism-and-sustainability\": \"{environmentalismandsustainability}\",\n",
    "  \"environmentalism-and-sustainability-confidence\": \"{environmentalismandsustainabilityconfidence}\",\n",
    "  \"gaming-and-e-sports\": \"{gamingandesports}\",\n",
    "  \"gaming-and-e-sports-confidence\": \"{gamingandesportsconfidence}\",\n",
    "  \"college-and-career\": \"{collegeandcareer}\",\n",
    "  \"college-and-career-confidence\": \"{collegeandcareerconfidence}\",\n",
    "  \"cooking-and-food\": \"{cookingandfood}\",\n",
    "  \"cooking-and-food-confidence\": \"{cookingandfoodconfidence}\",\n",
    "  \"reading-and-literature\": \"{readingandliterature}\",\n",
    "  \"reading-and-literature-confidence\": \"{readingandliteratureconfidence}\",\n",
    "  \"writing-and-creative-writing\": \"{writingandcreativewriting}\",\n",
    "  \"writing-and-creative-writing-confidence\": \"{writingandcreativewritingconfidence}\",\n",
    "  \"science-and-technology\": \"{scienceandtechnology}\",\n",
    "  \"science-and-technology-confidence\": \"{scienceandtechnologyconfidence}\",\n",
    "  \"mathematics-and-statistics\": \"{mathematicsandstatistics}\",\n",
    "  \"mathematics-and-statistics-confidence\": \"{mathematicsandstatisticsconfidence}\",\n",
    "  \"history-and-social-studies\": \"{historyandsocialstudies}\",\n",
    "  \"history-and-social-studies-confidence\": \"{historyandsocialstudiesconfidence}\",\n",
    "  \"creative-arts\": \"{creativearts}\",\n",
    "  \"creative-arts-confidence\": \"{creativeartsconfidence}\",\n",
    "  \"animals-and-nature\": \"{animalsandnature}\",\n",
    "  \"animals-and-nature-confidence\": \"{animalsandnatureconfidence}\",\n",
    "  \"note\": \"{note}\"\n",
    "}}\"\"\".format(sportsandathletics=data['Variables']['sports-and-athletics'],\n",
    "            sportsandathleticsconfidence=data['Variables']['sports-and-athletics-confidence'],\n",
    "            environmentalismandsustainability=data['Variables']['environmentalism-and-sustainability'],\n",
    "            environmentalismandsustainabilityconfidence=data['Variables']['environmentalism-and-sustainability-confidence'],\n",
    "            gamingandesports=data['Variables']['gaming-and-e-sports'],\n",
    "            gamingandesportsconfidence=data['Variables']['gaming-and-e-sports-confidence'],\n",
    "            collegeandcareer=data['Variables']['college-and-career'],\n",
    "            collegeandcareerconfidence=data['Variables']['college-and-career-confidence'],\n",
    "            cookingandfood=data['Variables']['cooking-and-food'],\n",
    "            cookingandfoodconfidence=data['Variables']['cooking-and-food-confidence'],\n",
    "            readingandliterature=data['Variables']['reading-and-literature'],\n",
    "            readingandliteratureconfidence=data['Variables']['reading-and-literature-confidence'],\n",
    "            writingandcreativewriting=data['Variables']['writing-and-creative-writing'],\n",
    "            writingandcreativewritingconfidence=data['Variables']['writing-and-creative-writing-confidence'],\n",
    "            scienceandtechnology=data['Variables']['science-and-technology'],\n",
    "            scienceandtechnologyconfidence=data['Variables']['science-and-technology-confidence'],\n",
    "            mathematicsandstatistics=data['Variables']['mathematics-and-statistics'],\n",
    "            mathematicsandstatisticsconfidence=data['Variables']['mathematics-and-statistics-confidence'],\n",
    "            historyandsocialstudies=data['Variables']['history-and-social-studies'],\n",
    "            historyandsocialstudiesconfidence=data['Variables']['history-and-social-studies-confidence'],\n",
    "            creativearts=data['Variables']['creative-arts'],\n",
    "            creativeartsconfidence=data['Variables']['creative-arts-confidence'],\n",
    "            animalsandnature=data['Variables']['animals-and-nature'],\n",
    "            animalsandnatureconfidence=data['Variables']['animals-and-nature-confidence'],\n",
    "            note=data['Note']\n",
    ")\n",
    "    \n",
    "    return solution\n",
    "\n",
    "def formatting_func(data):\n",
    "    return question_func(data)+solution_func(data)\n",
    "    \n",
    "print(formatting_func(data.loc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = data.apply(formatting_func, axis=1)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_data, val_data = train_test_split(documents, test_size=0.2, random_state=42)\n",
    "\n",
    "train_df = pd.DataFrame(train_data)\n",
    "train_df.columns = ['document']\n",
    "val_df = pd.DataFrame(val_data)\n",
    "val_df.columns = ['document']\n",
    "\n",
    "train_df.to_json('train_data.jsonl', orient='records', lines=True)\n",
    "val_df.to_json('validation_data.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Loading Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "s6f4z8EYmcJ6"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset(\"json\", data_files='./train_data.jsonl', split='train')\n",
    "eval_dataset = load_dataset(\"json\", data_files='./validation_data.jsonl', split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "shz8Xdv-yRgf"
   },
   "source": [
    "### 2. Load Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJ-5idQwzvg-"
   },
   "source": [
    "Let's now load Mistral - mistralai/Mistral-7B-v0.1 - using 4-bit quantization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "45524c98039a46d5b7745ad7cb638d2f"
     ]
    },
    "id": "E0Nl5mWL0k2T",
    "outputId": "47b6b01d-e9f2-4b70-919c-17ae64993843"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55877f6454bf4b16996c942abaef1478",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "base_model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjNdXolqyRgf"
   },
   "source": [
    "### 3. Tokenization\n",
    "\n",
    "Set up the tokenizer. Add padding on the left as it [makes training use less memory](https://ai.stackexchange.com/questions/41485/while-fine-tuning-a-decoder-only-llm-like-llama-on-chat-dataset-what-kind-of-pa).\n",
    "\n",
    "\n",
    "For `model_max_length`, it's helpful to get a distribution of your data lengths. Let's first tokenize without the truncation/padding, so we can get a length distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "haSUDD9HyRgf",
    "outputId": "22ee95db-2974-4ab0-e0c7-444d04d3e838"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True,\n",
    "    add_bos_token=True,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_prompt(prompt):\n",
    "    return tokenizer(prompt['document'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHnKLcq4yRgg"
   },
   "source": [
    "Reformat the prompt and tokenize each sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "S3iLAwLh3m19"
   },
   "outputs": [],
   "source": [
    "tokenized_train_dataset = train_dataset.map(tokenize_prompt)\n",
    "tokenized_val_dataset = eval_dataset.map(tokenize_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6ewk27p3m19"
   },
   "source": [
    "Let's get a distribution of our dataset lengths, so we can determine the appropriate `max_length` for our input tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "BA8M9yfC3m19",
    "outputId": "99c6d302-9bb6-47b1-cae9-a1cd870b4770"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset):\n",
    "    lengths = [len(x['input_ids']) for x in tokenized_train_dataset]\n",
    "    lengths += [len(x['input_ids']) for x in tokenized_val_dataset]\n",
    "\n",
    "    # Plotting the histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(lengths, bins=20, alpha=0.7, color='blue')\n",
    "    plt.xlabel('Length of input_ids')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Lengths of input_ids')\n",
    "    plt.show()\n",
    "\n",
    "plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBk4Qp_vyRgh"
   },
   "source": [
    "From here, you can choose where you'd like to set the `max_length` to be. You can truncate and pad training examples to fit them to your chosen size. Be aware that choosing a larger `max_length` has its compute tradeoffs.\n",
    "\n",
    "I'm using my personal notes to train the model, and they vary greatly in length. I spent some time cleaning the dataset so the samples were about the same length, cutting up individual notes if needed, but being sure to not cut in the middle of a word or sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bMlw8h743m19"
   },
   "source": [
    "Now let's tokenize again with padding and truncation, and set up the tokenize function to make labels and input_ids the same. This is basically what [self-supervised fine-tuning is](https://neptune.ai/blog/self-supervised-learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "acINaViR3m19"
   },
   "outputs": [],
   "source": [
    "max_length = 5000 # This was an appropriate max length for my dataset\n",
    "\n",
    "def generate_and_tokenize_prompt2(prompt):\n",
    "    result = tokenizer(\n",
    "        prompt['document'],\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "518d4f0b89bf4d57bf00d4c6d6e59eb5"
     ]
    },
    "id": "lTk-aTog3m19",
    "outputId": "4fb637b4-77a2-47c6-de7b-4fb620663dd7"
   },
   "outputs": [],
   "source": [
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt2)\n",
    "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQL796OayRgh"
   },
   "source": [
    "Check that `input_ids` is padded on the left with the `eos_token` (2) and there is an `eos_token` 2 added to the end, and the prompt starts with a `bos_token` (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "OKHhvxK83m19",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 774, 22478, 28747, 13, 1976, 460, 264, 15965, 6353, 24066, 1094, 8910, 3506, 6202, 298, 1760, 12749, 28725, 2490, 7575, 5066, 404, 28725, 4228, 17221, 17736, 28725, 8207, 11790, 3117, 28725, 4228, 19412, 8942, 1583, 28725, 304, 5202, 12426, 28723, 3604, 6258, 13640, 349, 298, 1424, 294, 26344, 15627, 272, 3270, 6355, 302, 524, 28733, 28740, 28750, 3567, 304, 9051, 2948, 4073, 3117, 5202, 298, 652, 10299, 304, 1455, 594, 28723, 1263, 1430, 10248, 24492, 28725, 3084, 264, 9292, 1928, 8707, 28747, 13, 13, 19167, 636, 28747, 330, 1192, 302, 28705, 28740, 325, 335, 272, 24492, 349, 2169, 28731, 442, 28705, 28734, 325, 335, 459, 609, 3655, 390, 28705, 28740, 1019, 513, 865, 744, 302, 272, 1178, 8753, 28713, 395, 396, 24492, 28723, 13, 10193, 3164, 28747, 7133, 547, 264, 9843, 2184, 356, 264, 5657, 302, 28705, 28740, 28733, 28740, 28734, 298, 11634, 574, 2184, 302, 2552, 884, 297, 272, 5643, 28723, 13, 12205, 28747, 560, 12856, 1871, 356, 272, 11408, 1307, 298, 7964, 369, 2552, 4073, 3117, 654, 10248, 304, 264, 14060, 302, 272, 28649, 4686, 6355, 28723, 13, 13, 21432, 1184, 11533, 297, 272, 1178, 28725, 459, 776, 3235, 15321, 1927, 28723, 13, 13, 21432, 1184, 369, 272, 3270, 6355, 5016, 1002, 477, 264, 2052, 28733, 815, 2465, 3895, 28723, 13, 13, 3381, 25787, 10741, 938, 574, 1489, 16548, 28725, 7967, 18110, 297, 9843, 7420, 28723, 13, 13, 15423, 460, 272, 2948, 4073, 3117, 354, 456, 3638, 8230, 13, 28801, 13, 28742, 28713, 2729, 28733, 391, 28733, 498, 895, 1063, 27732, 13, 28742, 21680, 282, 1443, 28733, 391, 28733, 28713, 27301, 2437, 27732, 13, 28742, 28721, 6133, 28733, 391, 28733, 28706, 28733, 28713, 2729, 27732, 13, 28742, 1396, 3528, 28733, 391, 28733, 8001, 263, 27732, 13, 28742, 27177, 288, 28733, 391, 28733, 18972, 27732, 13, 28742, 923, 288, 28733, 391, 28733, 24477, 1373, 27732, 13, 28742, 19061, 28733, 391, 28733, 16346, 495, 28733, 19061, 27732, 13, 28742, 17819, 28733, 391, 28733, 23207, 2161, 27732, 13, 28742, 3076, 11953, 1063, 28733, 391, 28733, 5975, 5513, 27732, 13, 28742, 16346, 495, 28733, 8060, 27732, 13, 28742, 12714, 973, 28733, 391, 28733, 28711, 1373, 27732, 13, 28742, 15205, 28733, 391, 28733, 22698, 28733, 13082, 497, 27732, 13, 13, 13, 27332, 11147, 5284, 28747, 13, 1410, 4736, 14672, 647, 464, 28768, 357, 11042, 7871, 3117, 647, 464, 28768, 357, 11042, 7871, 3117, 304, 27911, 647, 464, 28768, 357, 11042, 7871, 3117, 464, 28793, 13, 13, 27332, 16693, 15985, 28747, 13, 4441, 13, 28705, 345, 28713, 2729, 28733, 391, 28733, 498, 895, 1063, 1264, 345, 28740, 10939, 13, 28705, 345, 28713, 2729, 28733, 391, 28733, 498, 895, 1063, 28733, 5552, 3164, 1264, 345, 28784, 10939, 13, 28705, 345, 21680, 282, 1443, 28733, 391, 28733, 28713, 27301, 2437, 1264, 345, 28734, 10939, 13, 28705, 345, 21680, 282, 1443, 28733, 391, 28733, 28713, 27301, 2437, 28733, 5552, 3164, 1264, 345, 28783, 10939, 13, 28705, 345, 28721, 6133, 28733, 391, 28733, 28706, 28733, 28713, 2729, 1264, 345, 28734, 10939, 13, 28705, 345, 28721, 6133, 28733, 391, 28733, 28706, 28733, 28713, 2729, 28733, 5552, 3164, 1264, 345, 28740, 28734, 10939, 13, 28705, 345, 1396, 3528, 28733, 391, 28733, 8001, 263, 1264, 345, 28740, 10939, 13, 28705, 345, 1396, 3528, 28733, 391, 28733, 8001, 263, 28733, 5552, 3164, 1264, 345, 28787, 10939, 13, 28705, 345, 27177, 288, 28733, 391, 28733, 18972, 1264, 345, 28740, 10939, 13, 28705, 345, 27177, 288, 28733, 391, 28733, 18972, 28733, 5552, 3164, 1264, 345, 28787, 10939, 13, 28705, 345, 923, 288, 28733, 391, 28733, 24477, 1373, 1264, 345, 28734, 10939, 13, 28705, 345, 923, 288, 28733, 391, 28733, 24477, 1373, 28733, 5552, 3164, 1264, 345, 28787, 10939, 13, 28705, 345, 19061, 28733, 391, 28733, 16346, 495, 28733, 19061, 1264, 345, 28740, 10939, 13, 28705, 345, 19061, 28733, 391, 28733, 16346, 495, 28733, 19061, 28733, 5552, 3164, 1264, 345, 28783, 10939, 13, 28705, 345, 17819, 28733, 391, 28733, 23207, 2161, 1264, 345, 28734, 10939, 13, 28705, 345, 17819, 28733, 391, 28733, 23207, 2161, 28733, 5552, 3164, 1264, 345, 28740, 28734, 10939, 13, 28705, 345, 3076, 11953, 1063, 28733, 391, 28733, 5975, 5513, 1264, 345, 28740, 10939, 13, 28705, 345, 3076, 11953, 1063, 28733, 391, 28733, 5975, 5513, 28733, 5552, 3164, 1264, 345, 28784, 10939, 13, 28705, 345, 16346, 495, 28733, 8060, 1264, 345, 28740, 10939, 13, 28705, 345, 16346, 495, 28733, 8060, 28733, 5552, 3164, 1264, 345, 28783, 10939, 13, 28705, 345, 12714, 973, 28733, 391, 28733, 28711, 1373, 1264, 345, 28740, 10939, 13, 28705, 345, 12714, 973, 28733, 391, 28733, 28711, 1373, 28733, 5552, 3164, 1264, 345, 28774, 10939, 13, 28705, 345, 15205, 28733, 391, 28733, 22698, 28733, 13082, 497, 1264, 345, 28734, 10939, 13, 28705, 345, 15205, 28733, 391, 28733, 22698, 28733, 13082, 497, 28733, 5552, 3164, 1264, 345, 28774, 10939, 13, 28705, 345, 8838, 1264, 345, 5155, 2384, 23276, 3187, 274, 4003, 345, 28801, 13, 28752, 13, 13, 27332, 27786, 28747, 13, 28751, 13, 28705, 345, 28713, 2729, 28733, 391, 28733, 498, 895, 1063, 1264, 345, 28740, 28723, 28734, 548, 13, 28705, 345, 28713, 2729, 28733, 391, 28733, 498, 895, 1063, 28733, 5552, 3164, 1264, 345, 28774, 28723, 28734, 548, 13, 28705, 345, 21680, 282, 1443, 28733, 391, 28733, 28713, 27301, 2437, 1264, 345, 28734, 28723, 28734, 548, 13, 28705, 345, 21680, 282, 1443, 28733, 391, 28733, 28713, 27301, 2437, 28733, 5552, 3164, 1264, 345, 28740, 28734, 28723, 28734, 548, 13, 28705, 345, 28721, 6133, 28733, 391, 28733, 28706, 28733, 28713, 2729, 1264, 345, 28734, 28723, 28734, 548, 13, 28705, 345, 28721, 6133, 28733, 391, 28733, 28706, 28733, 28713, 2729, 28733, 5552, 3164, 1264, 345, 28740, 28734, 28723, 28734, 548, 13, 28705, 345, 1396, 3528, 28733, 391, 28733, 8001, 263, 1264, 345, 28734, 28723, 28734, 548, 13, 28705, 345, 1396, 3528, 28733, 391, 28733, 8001, 263, 28733, 5552, 3164, 1264, 345, 28740, 28734, 28723, 28734, 548, 13, 28705, 345, 27177, 288, 28733, 391, 28733, 18972, 1264, 345, 28734, 28723, 28734, 548, 13, 28705, 345, 27177, 288, 28733, 391, 28733, 18972, 28733, 5552, 3164, 1264, 345, 28740, 28734, 28723, 28734, 548, 13, 28705, 345, 923, 288, 28733, 391, 28733, 24477, 1373, 1264, 345, 28734, 28723, 28734, 548, 13, 28705, 345, 923, 288, 28733, 391, 28733, 24477, 1373, 28733, 5552, 3164, 1264, 345, 28740, 28734, 28723, 28734, 548, 13, 28705, 345, 19061, 28733, 391, 28733, 16346, 495, 28733, 19061, 1264, 345, 28734, 28723, 28734, 548, 13, 28705, 345, 19061, 28733, 391, 28733, 16346, 495, 28733, 19061, 28733, 5552, 3164, 1264, 345, 28740, 28734, 28723, 28734, 548, 13, 28705, 345, 17819, 28733, 391, 28733, 23207, 2161, 1264, 345, 28734, 28723, 28734, 548, 13, 28705, 345, 17819, 28733, 391, 28733, 23207, 2161, 28733, 5552, 3164, 1264, 345, 28740, 28734, 28723, 28734, 548, 13, 28705, 345, 3076, 11953, 1063, 28733, 391, 28733, 5975, 5513, 1264, 345, 28734, 28723, 28734, 548, 13, 28705, 345, 3076, 11953, 1063, 28733, 391, 28733, 5975, 5513, 28733, 5552, 3164, 1264, 345, 28740, 28734, 28723, 28734, 548, 13, 28705, 345, 15205, 28733, 391, 28733, 22698, 28733, 13082, 497, 1264, 345, 28734, 28723, 28734, 548, 13, 28705, 345, 15205, 28733, 391, 28733, 22698, 28733, 13082, 497, 28733, 5552, 3164, 1264, 345, 28740, 28734, 28723, 28734, 548, 13, 28705, 345, 16346, 495, 28733, 8060, 1264, 345, 28734, 28723, 28734, 548, 13, 28705, 345, 16346, 495, 28733, 8060, 28733, 5552, 3164, 1264, 345, 28740, 28734, 28723, 28734, 548, 13, 28705, 345, 12714, 973, 28733, 391, 28733, 28711, 1373, 1264, 345, 28740, 28723, 28734, 548, 13, 28705, 345, 12714, 973, 28733, 391, 28733, 28711, 1373, 28733, 5552, 3164, 1264, 345, 28783, 28723, 28734, 548, 13, 28705, 345, 8838, 1264, 345, 1014, 5716, 6642, 264, 3081, 2145, 297, 8657, 28725, 10107, 14672, 28725, 14075, 486, 264, 1863, 3472, 28723, 851, 349, 10727, 395, 1486, 9843, 28723, 16569, 28725, 736, 654, 5166, 15321, 1927, 5202, 298, 461, 12736, 1168, 28725, 652, 7871, 3117, 28725, 304, 27911, 28725, 690, 12308, 396, 2145, 297, 8222, 304, 4735, 28723, 415, 9332, 4735, 302, 1167, 15321, 1927, 12095, 9843, 297, 456, 2145, 28723, 1387, 654, 708, 4073, 3117, 302, 272, 799, 13187, 297, 272, 3857, 1178, 611, 13, 28752, 2]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train_dataset[1]['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6LRa2Zm3m19"
   },
   "source": [
    "Now all the samples should be the same length, `max_length`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(min([len(x['input_ids']) for x in tokenized_train_dataset]) == max([len(x['input_ids']) for x in tokenized_train_dataset]))\n",
    "assert(min([len(x['input_ids']) for x in tokenized_val_dataset]) == max([len(x['input_ids']) for x in tokenized_val_dataset]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "I55Yo3yy3m19",
    "outputId": "c87e344d-e0f3-4542-afcc-4e2025926d64"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1IAAAIhCAYAAABE54vcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABHRUlEQVR4nO3daVxVVf////eRGQQUEJDEIefEIbVMs8QULcfULvWyNE2/2aWplGaDldggaeVQlk2mlqamSWl6eYljmlZO5JCZmWOCNhjgECDs/w3/nF9HQFkInKO+no/HvnHWXnvvzz5nobzZe69jsyzLEgAAAACg0Mo4uwAAAAAAuNoQpAAAAADAEEEKAAAAAAwRpAAAAADAEEEKAAAAAAwRpAAAAADAEEEKAAAAAAwRpAAAAADAEEEKAAAAAAwRpABcM2bNmiWbzWZfvL29FR4ertatWys+Pl4nT57Ms01cXJxsNpvRcc6ePau4uDitW7fOaLv8jlW1alV16tTJaD+X88knn2jKlCn5rrPZbIqLiyvW4xW31atXq2nTpvLz85PNZtPnn3+eb79Dhw7JZrPptddeK90CDYwfPz7f+nPH6tatW0u/qHw8++yzqly5stzd3VWuXLkC+xXl56UkHT9+XHFxcUpKSjLeNnf8zJo167J9Xe28AbgGghSAa87MmTO1efNmJSYm6q233lKjRo00YcIE1a1bV6tWrXLoO2jQIG3evNlo/2fPntW4ceOMg1RRjlUUlwpSmzdv1qBBg0q8hqKyLEs9e/aUh4eHlixZos2bN6tVq1bOLqvICgpSruSLL77Qyy+/rH79+mn9+vV5fkb+qbTGcGEdP35c48aNK1KQqlixojZv3qyOHTsWf2EArgvuzi4AAIpbVFSUmjZtan/do0cPPfbYY2rZsqW6d++u/fv3KywsTJJUqVIlVapUqUTrOXv2rHx9fUvlWJdz2223OfX4l3P8+HH9+eef6tatm9q0aePscq4Lu3fvliQNHz5coaGhl+zrCmO4uHh5ebn8zwMA18YVKQDXhcqVK+v1119Xenq63n33XXt7frfsrFmzRtHR0QoODpaPj48qV66sHj166OzZszp06JAqVKggSRo3bpz9NsL+/fs77G/79u267777VL58eVWvXr3AY+VKSEhQgwYN5O3trRtvvFFvvPGGw/rcW8EOHTrk0L5u3TrZbDb71bHo6GgtW7ZMhw8fdrjNMVd+t/bt3r1bXbt2Vfny5eXt7a1GjRpp9uzZ+R5n3rx5GjNmjCIiIhQQEKC2bdtq3759Bb/x/7Bx40a1adNG/v7+8vX1VYsWLbRs2TL7+ri4OPsv6U8++aRsNpuqVq1aqH1fSlpamkaNGqVq1arJ09NTN9xwg2JjY3XmzBmHfjabTY8++qg+/vhj1a1bV76+vmrYsKG+/PLLPPv84osv1KBBA3l5eenGG2/U1KlT83y+NptNZ86c0ezZs+2fQ3R0tMN+0tPT9Z///EchISEKDg5W9+7ddfz4cYc+lxqPl5KTk6OJEyeqTp068vLyUmhoqPr166djx47Z+1StWlXPPvusJCksLOyyt35e6vbUFStWqHHjxvLx8VGdOnX04YcfOvTLHcOJiYkaMGCAgoKC5Ofnp86dO+uXX37Js8/cn6l/io6Otr+H69at0y233CJJGjBggP09LuytqwXd2rds2TI1atRIXl5eqlatWoG3ji5cuFDNmjVTYGCgfH19deONN+qhhx4q1LEBXBu4IgXgutGhQwe5ubnpq6++KrDPoUOH1LFjR91xxx368MMPVa5cOf36669asWKFMjMzVbFiRa1YsUJ33323Bg4caL9NLjdc5erevbt69+6tRx55JM8v7BdLSkpSbGys4uLiFB4errlz52rEiBHKzMzUqFGjjM7x7bff1sMPP6wDBw4oISHhsv337dunFi1aKDQ0VG+88YaCg4M1Z84c9e/fXydOnNDo0aMd+j/zzDO6/fbb9cEHHygtLU1PPvmkOnfurL1798rNza3A46xfv14xMTFq0KCBZsyYIS8vL7399tvq3Lmz5s2bp169emnQoEFq2LChunfvrmHDhqlPnz7y8vIyOv+LnT17Vq1atdKxY8f0zDPPqEGDBtqzZ4+ef/557dq1S6tWrXIIBsuWLdOWLVv0wgsvqGzZspo4caK6deumffv26cYbb5QkrVixQt27d9edd96pBQsW6Pz583rttdd04sQJh2Nv3rxZd911l1q3bq3nnntOkhQQEODQZ9CgQerYsaM++eQTHT16VE888YQeeOABrVmzRtLlx6Ovr2+B5/6f//xH7733nh599FF16tRJhw4d0nPPPad169Zp+/btCgkJUUJCgt566y3NmDFDK1asUGBgYJGuOH3//fcaOXKknnrqKYWFhemDDz7QwIEDVaNGDd15550OfQcOHKiYmBj7OT/77LOKjo7Wzp07L/l81sUaN26smTNnasCAAXr22Wftt+hdyRWz1atXq2vXrmrevLnmz5+v7OxsTZw4Md/PtlevXurVq5fi4uLk7e2tw4cP2z83ANcJCwCuETNnzrQkWVu2bCmwT1hYmFW3bl3767Fjx1r//Kdw0aJFliQrKSmpwH389ttvliRr7Nixedbl7u/5558vcN0/ValSxbLZbHmOFxMTYwUEBFhnzpxxOLeDBw869Fu7dq0lyVq7dq29rWPHjlaVKlXyrf3iunv37m15eXlZR44cceh3zz33WL6+vtZff/3lcJwOHTo49Pv0008tSdbmzZvzPV6u2267zQoNDbXS09PtbefPn7eioqKsSpUqWTk5OZZlWdbBgwctSdarr756yf0Vtm98fLxVpkyZPGMi93Nevny5vU2SFRYWZqWlpdnbUlJSrDJlyljx8fH2tltuucWKjIy0MjIy7G3p6elWcHBwns/Xz8/PevDBB/PUlft5DhkyxKF94sSJliQrOTnZoc5Ljcf87N27N9/9f/vtt5Yk65lnnrG35Y7L33777bL7LWgMe3t7W4cPH7a3nTt3zgoKCrIGDx5sb8s9527dujls//XXX1uSrJdeeslhn/m9b61atbJatWplf71lyxZLkjVz5szL1n6x3PHzz22bNWtmRUREWOfOnbO3paWlWUFBQQ7n/dprr1mS7D8fAK5P3NoH4LpiWdYl1zdq1Eienp56+OGHNXv27Dy3HBVWjx49Ct23Xr16atiwoUNbnz59lJaWpu3btxfp+IW1Zs0atWnTRpGRkQ7t/fv319mzZ/NMLNClSxeH1w0aNJAkHT58uMBjnDlzRt9++63uu+8+lS1b1t7u5uamvn376tixY4W+PdDUl19+qaioKDVq1Ejnz5+3L+3bt3e4JTJX69at5e/vb38dFham0NBQ+/mdOXNGW7du1b333itPT097v7Jly6pz587G9V3u/SzqeFy7dq0k5bk97tZbb1XdunW1evVq41ovpVGjRqpcubL9tbe3t2rVqpXvuLj//vsdXrdo0UJVqlSx1+wsZ86c0ZYtW9S9e3d5e3vb2/39/fN8trm3FPbs2VOffvqpfv3111KtFYBrIEgBuG6cOXNGf/zxhyIiIgrsU716da1atUqhoaEaOnSoqlevrurVq2vq1KlGx6pYsWKh+4aHhxfY9scffxgd19Qff/yRb62579HFxw8ODnZ4nXvr3blz5wo8xqlTp2RZltFxisuJEye0c+dOeXh4OCz+/v6yLEu///67Q/+Lz0+6cI6555d7LrmTlfxTfm2Xc7n3s6jjMff9LOg9L+73+3Lv2z8VNN5LeqxfzqlTp5STk3PJn8dcd955pz7//HOdP39e/fr1U6VKlRQVFaV58+aVVrkAXADPSAG4bixbtkzZ2dl5Hvi/2B133KE77rhD2dnZ2rp1q958803FxsYqLCxMvXv3LtSxTL5zJiUlpcC23F9Qc/9CnpGR4dDv4iBgKjg4WMnJyXnacyc8CAkJuaL9S1L58uVVpkyZEj9OfkJCQuTj45Nn4oN/rjdRvnx52Wy2PM/MSPl/jsWhKOMxd9wkJyfneWbo+PHjJfZ+F0ZB471GjRr2197e3nnGunRhvJdU7bmf7aV+Hv+pa9eu6tq1qzIyMvTNN98oPj5effr0UdWqVdW8efMSqRGAa+GKFIDrwpEjRzRq1CgFBgZq8ODBhdrGzc1NzZo101tvvSVJ9tvsCnMVxsSePXv0/fffO7R98skn8vf3V+PGjSXJPnvdzp07HfotWbIkz/4KuhKQnzZt2mjNmjV5Zor76KOP5OvrWyzTQ/v5+alZs2ZavHixQ105OTmaM2eOKlWqpFq1al3xcfLTqVMnHThwQMHBwWratGmexXRWQD8/PzVt2lSff/65MjMz7e2nT5/Od3Y/k8/icgoaj/m56667JElz5sxxaN+yZYv27t3r1Knl586d6/B606ZNOnz4sMMfOKpWrZpnrP/00095bgEtzp9FPz8/3XrrrVq8eLH+/vtve3t6erqWLl1a4HZeXl5q1aqVJkyYIEnasWPHFdcC4OrAFSkA15zdu3fbn4U5efKkNmzYoJkzZ8rNzU0JCQl5Ztj7p3feeUdr1qxRx44dVblyZf3999/2qxlt27aVdOGZiSpVquiLL75QmzZtFBQUpJCQkCJP1R0REaEuXbooLi5OFStW1Jw5c5SYmKgJEybYZ2W75ZZbVLt2bY0aNUrnz59X+fLllZCQoI0bN+bZX/369bV48WJNnz5dTZo0UZkyZRy+V+ufxo4dqy+//FKtW7fW888/r6CgIM2dO1fLli3TxIkTFRgYWKRzulh8fLxiYmLUunVrjRo1Sp6ennr77be1e/duzZs3z+gK3sV27dqlRYsW5Wm/5ZZbFBsbq88++0x33nmnHnvsMTVo0EA5OTk6cuSIVq5cqZEjR6pZs2ZGx3vhhRfUsWNHtW/fXiNGjFB2drZeffVVlS1bVn/++adD3/r162vdunVaunSpKlasKH9/f9WuXbvQxyrMeMxP7dq19fDDD+vNN99UmTJldM8999hn7YuMjNRjjz1mdM7FaevWrRo0aJD+9a9/6ejRoxozZoxuuOEGDRkyxN6nb9++euCBBzRkyBD16NFDhw8f1sSJE/P87FavXl0+Pj6aO3eu6tatq7JlyyoiIuKSt+9eyosvvqi7775bMTExGjlypLKzszVhwgT5+fk5fLbPP/+8jh07pjZt2qhSpUr666+/NHXqVHl4eFzVXyANwJBz57oAgOKTOytY7uLp6WmFhoZarVq1ssaPH2+dPHkyzzYXz0K2efNmq1u3blaVKlUsLy8vKzg42GrVqpW1ZMkSh+1WrVpl3XzzzZaXl5clyT7D2KVmQCtoxrOOHTtaixYtsurVq2d5enpaVatWtSZNmpRn+59++slq166dFRAQYFWoUMEaNmyYtWzZsjyz9v3555/WfffdZ5UrV86y2WwOx1Q+sw3u2rXL6ty5sxUYGGh5enpaDRs2zDMLWu6sfQsXLnRoz2/ms4Js2LDBuuuuuyw/Pz/Lx8fHuu2226ylS5fmuz+TWfsKWnJrOn36tPXss89atWvXtjw9Pa3AwECrfv361mOPPWalpKQ4vDdDhw7Nc5z8ZpBLSEiw6tevb3l6elqVK1e2XnnlFWv48OFW+fLlHfolJSVZt99+u+Xr62tJss84V9AMkxfPwljY8Zif7Oxsa8KECVatWrUsDw8PKyQkxHrggQeso0ePOvQrjln7OnbsmKfvxTPs5Z7zypUrrb59+1rlypWzfHx8rA4dOlj79+932DYnJ8eaOHGideONN1re3t5W06ZNrTVr1uTZp2VZ1rx586w6depYHh4eBc6mmZ+Cxu6SJUusBg0aOHy2F5/3l19+ad1zzz3WDTfcYP93pkOHDtaGDRsKdWwA1wabZV1mCisAAHBJWVlZatSokW644QatXLnS2eW4pFmzZmnAgAHasmVLgVdIAeBqwq19AAAYyv1S2YoVKyolJUXvvPOO9u7dazy7IwDg6kWQAgDAUHp6ukaNGqXffvtNHh4eaty4sZYvX37J55ZQOizLUnZ29iX7uLm5XdFzeQAgSdzaBwAArhnr1q1T69atL9ln5syZeb6sGABMEaQAAMA1Iz09Pc806RerVq1avl8iDAAmCFIAAAAAYIgv5AUAAAAAQ0w2ISknJ0fHjx+Xv78/D58CAAAA1zHLspSenq6IiAiVKVPwdSeClKTjx48rMjLS2WUAAAAAcBFHjx5VpUqVClxPkJLk7+8v6cKbFRAQ4ORqAAAAADhLWlqaIiMj7RmhIAQpyX47X0BAAEEKAAAAwGUf+WGyCQAAAAAwRJACAAAAAEMEKQAAAAAwRJACAAAAAEMEKQAAAAAwRJACAAAAAEMEKQAAAAAwRJACAAAAAEMEKQAAAAAwRJACAAAAAEMEKQAAAAAwRJACAAAAAEMEKQAAAAAwRJACAAAAAEMEKQAAAAAw5NQgFRcXJ5vN5rCEh4fb11uWpbi4OEVERMjHx0fR0dHas2ePwz4yMjI0bNgwhYSEyM/PT126dNGxY8dK+1QAAAAAXEecfkWqXr16Sk5Oti+7du2yr5s4caImTZqkadOmacuWLQoPD1dMTIzS09PtfWJjY5WQkKD58+dr48aNOn36tDp16qTs7GxnnA4AAACA64C70wtwd3e4CpXLsixNmTJFY8aMUffu3SVJs2fPVlhYmD755BMNHjxYqampmjFjhj7++GO1bdtWkjRnzhxFRkZq1apVat++fameCwAAAIDrg9OvSO3fv18RERGqVq2aevfurV9++UWSdPDgQaWkpKhdu3b2vl5eXmrVqpU2bdokSdq2bZuysrIc+kRERCgqKsreJz8ZGRlKS0tzWAAAAACgsJx6RapZs2b66KOPVKtWLZ04cUIvvfSSWrRooT179iglJUWSFBYW5rBNWFiYDh8+LElKSUmRp6enypcvn6dP7vb5iY+P17hx44r5bAAAV7vOnZ1dgaOlS51dAQCgIE69InXPPfeoR48eql+/vtq2batly5ZJunALXy6bzeawjWVZedoudrk+Tz/9tFJTU+3L0aNHr+AsAAAAAFxvnH5r3z/5+fmpfv362r9/v/25qYuvLJ08edJ+lSo8PFyZmZk6depUgX3y4+XlpYCAAIcFAAAAAArLpYJURkaG9u7dq4oVK6patWoKDw9XYmKifX1mZqbWr1+vFi1aSJKaNGkiDw8Phz7JycnavXu3vQ8AAAAAFDenPiM1atQode7cWZUrV9bJkyf10ksvKS0tTQ8++KBsNptiY2M1fvx41axZUzVr1tT48ePl6+urPn36SJICAwM1cOBAjRw5UsHBwQoKCtKoUaPstwoCAAAAQElwapA6duyY/v3vf+v3339XhQoVdNttt+mbb75RlSpVJEmjR4/WuXPnNGTIEJ06dUrNmjXTypUr5e/vb9/H5MmT5e7urp49e+rcuXNq06aNZs2aJTc3N2edFgAAAIBrnM2yLMvZRThbWlqaAgMDlZqayvNSAHAdY9Y+AEBhs4FLPSMFAAAAAFcDghQAAAAAGCJIAQAAAIAhghQAAAAAGCJIAQAAAIAhghQAAAAAGCJIAQAAAIAhghQAAAAAGCJIAQAAAIAhghQAAAAAGCJIAQAAAIAhghQAAAAAGCJIAQAAAIAhghQAAAAAGCJIAQAAAIAhghQAAAAAGCJIAQAAAIAhghQAAAAAGCJIAQAAAIAhghQAAAAAGCJIAQAAAIAhghQAAAAAGCJIAQAAAIAhghQAAAAAGCJIAQAAAIAhghQAAAAAGCJIAQAAAIAhghQAAAAAGCJIAQAAAIAhghQAAAAAGCJIAQAAAIAhghQAAAAAGCJIAQAAAIAhghQAAAAAGCJIAQAAAIAhghQAAAAAGCJIAQAAAIAhghQAAAAAGCJIAQAAAIAhghQAAAAAGCJIAQAAAIAhghQAAAAAGCJIAQAAAIAhghQAAAAAGCJIAQAAAIAhghQAAAAAGCJIAQAAAIAhghQAAAAAGCJIAQAAAIAhghQAAAAAGCJIAQAAAIAhghQAAAAAGCJIAQAAAIAhghQAAAAAGCJIAQAAAIAhghQAAAAAGCJIAQAAAIAhghQAAAAAGCJIAQAAAIAhghQAAAAAGCJIAQAAAIAhghQAAAAAGCJIAQAAAIAhghQAAAAAGCJIAQAAAIAhghQAAAAAGCJIAQAAAIAhghQAAAAAGCJIAQAAAIAhghQAAAAAGCJIAQAAAIAhghQAAAAAGCJIAQAAAIAhghQAAAAAGCJIAQAAAIAhghQAAAAAGCJIAQAAAIAhlwlS8fHxstlsio2NtbdZlqW4uDhFRETIx8dH0dHR2rNnj8N2GRkZGjZsmEJCQuTn56cuXbro2LFjpVw9AAAAgOuJSwSpLVu26L333lODBg0c2idOnKhJkyZp2rRp2rJli8LDwxUTE6P09HR7n9jYWCUkJGj+/PnauHGjTp8+rU6dOik7O7u0TwMAAADAdcLpQer06dO6//779f7776t8+fL2dsuyNGXKFI0ZM0bdu3dXVFSUZs+erbNnz+qTTz6RJKWmpmrGjBl6/fXX1bZtW918882aM2eOdu3apVWrVjnrlAAAAABc45wepIYOHaqOHTuqbdu2Du0HDx5USkqK2rVrZ2/z8vJSq1attGnTJknStm3blJWV5dAnIiJCUVFR9j75ycjIUFpamsMCAAAAAIXl7syDz58/X9u3b9eWLVvyrEtJSZEkhYWFObSHhYXp8OHD9j6enp4OV7Jy++Run5/4+HiNGzfuSssHAAAAcJ1y2hWpo0ePasSIEZozZ468vb0L7Gez2RxeW5aVp+1il+vz9NNPKzU11b4cPXrUrHgAAAAA1zWnBalt27bp5MmTatKkidzd3eXu7q7169frjTfekLu7u/1K1MVXlk6ePGlfFx4erszMTJ06darAPvnx8vJSQECAwwIAAAAAheW0INWmTRvt2rVLSUlJ9qVp06a6//77lZSUpBtvvFHh4eFKTEy0b5OZman169erRYsWkqQmTZrIw8PDoU9ycrJ2795t7wMAAAAAxc1pz0j5+/srKirKoc3Pz0/BwcH29tjYWI0fP141a9ZUzZo1NX78ePn6+qpPnz6SpMDAQA0cOFAjR45UcHCwgoKCNGrUKNWvXz/P5BUAAAAAUFycOtnE5YwePVrnzp3TkCFDdOrUKTVr1kwrV66Uv7+/vc/kyZPl7u6unj176ty5c2rTpo1mzZolNzc3J1YOAAAA4FpmsyzLcnYRzpaWlqbAwEClpqbyvBQAXMc6d3Z2BY6WLnV2BQBw/SlsNnD690gBAAAAwNWGIAUAAAAAhghSAAAAAGCIIAUAAAAAhghSAAAAAGCIIAUAAAAAhghSAAAAAGCIIAUAAAAAhghSAAAAAGCIIAUAAAAAhghSAAAAAGCIIAUAAAAAhghSAAAAAGCIIAUAAAAAhghSAAAAAGCIIAUAAAAAhghSAAAAAGCIIAUAAAAAhghSAAAAAGCIIAUAAAAAhghSAAAAAGCIIAUAAAAAhghSAAAAAGCIIAUAAAAAhghSAAAAAGCIIAUAAAAAhghSAAAAAGCIIAUAAAAAhghSAAAAAGCIIAUAAAAAhghSAAAAAGCIIAUAAAAAhghSAAAAAGCIIAUAAAAAhghSAAAAAGCIIAUAAAAAhghSAAAAAGCIIAUAAAAAhghSAAAAAGCIIAUAAAAAhghSAAAAAGCIIAUAAAAAhghSAAAAAGCIIAUAAAAAhghSAAAAAGCIIAUAAAAAhghSAAAAAGCIIAUAAAAAhghSAAAAAGCIIAUAAAAAhghSAAAAAGCIIAUAAAAAhghSAAAAAGCIIAUAAAAAhghSAAAAAGCIIAUAAAAAhghSAAAAAGCIIAUAAAAAhghSAAAAAGCIIAUAAAAAhghSAAAAAGCIIAUAAAAAhghSAAAAAGCIIAUAAAAAhghSAAAAAGCIIAUAAAAAhghSAAAAAGCIIAUAAAAAhghSAAAAAGCIIAUAAAAAhghSAAAAAGCIIAUAAAAAhghSAAAAAGCIIAUAAAAAhghSAAAAAGCIIAUAAAAAhghSAAAAAGDIqUFq+vTpatCggQICAhQQEKDmzZvrv//9r329ZVmKi4tTRESEfHx8FB0drT179jjsIyMjQ8OGDVNISIj8/PzUpUsXHTt2rLRPBQAAAMB1xKlBqlKlSnrllVe0detWbd26VXfddZe6du1qD0sTJ07UpEmTNG3aNG3ZskXh4eGKiYlRenq6fR+xsbFKSEjQ/PnztXHjRp0+fVqdOnVSdna2s04LAAAAwDXOZlmW5ewi/ikoKEivvvqqHnroIUVERCg2NlZPPvmkpAtXn8LCwjRhwgQNHjxYqampqlChgj7++GP16tVLknT8+HFFRkZq+fLlat++faGOmZaWpsDAQKWmpiogIKDEzg0A4No6d3Z2BY6WLnV2BQBw/SlsNnCZZ6Sys7M1f/58nTlzRs2bN9fBgweVkpKidu3a2ft4eXmpVatW2rRpkyRp27ZtysrKcugTERGhqKgoe5/8ZGRkKC0tzWEBAAAAgMIqUpA6ePBgsRWwa9culS1bVl5eXnrkkUeUkJCgm266SSkpKZKksLAwh/5hYWH2dSkpKfL09FT58uUL7JOf+Ph4BQYG2pfIyMhiOx8AAAAA174iBakaNWqodevWmjNnjv7+++8rKqB27dpKSkrSN998o//85z968MEH9cMPP9jX22w2h/6WZeVpu9jl+jz99NNKTU21L0ePHr2icwAAAABwfSlSkPr+++918803a+TIkQoPD9fgwYP13XffFakAT09P1ahRQ02bNlV8fLwaNmyoqVOnKjw8XJLyXFk6efKk/SpVeHi4MjMzderUqQL75MfLy8s+U2DuAgAAAACFVaQgFRUVpUmTJunXX3/VzJkzlZKSopYtW6pevXqaNGmSfvvttyIXZFmWMjIyVK1aNYWHhysxMdG+LjMzU+vXr1eLFi0kSU2aNJGHh4dDn+TkZO3evdveBwAAAACK2xVNNuHu7q5u3brp008/1YQJE3TgwAGNGjVKlSpVUr9+/ZScnHzJ7Z955hlt2LBBhw4d0q5duzRmzBitW7dO999/v2w2m2JjYzV+/HglJCRo9+7d6t+/v3x9fdWnTx9JUmBgoAYOHKiRI0dq9erV2rFjhx544AHVr19fbdu2vZJTAwAAAIACuV/Jxlu3btWHH36o+fPny8/PT6NGjdLAgQN1/PhxPf/88+rateslb/k7ceKE+vbtq+TkZAUGBqpBgwZasWKFYmJiJEmjR4/WuXPnNGTIEJ06dUrNmjXTypUr5e/vb9/H5MmT5e7urp49e+rcuXNq06aNZs2aJTc3tys5NQAAAAAoUJG+R2rSpEmaOXOm9u3bpw4dOmjQoEHq0KGDypT5fxe4fv75Z9WpU0fnz58v1oJLAt8jBQCQ+B4pAEDhs0GRrkhNnz5dDz30kAYMGGCfFOJilStX1owZM4qyewAAAABwaUUKUvv3779sH09PTz344INF2T0AAAAAuLQiTTYxc+ZMLVy4ME/7woULNXv27CsuCgAAAABcWZGC1CuvvKKQkJA87aGhoRo/fvwVFwUAAAAArqxIQerw4cOqVq1anvYqVaroyJEjV1wUAAAAALiyIgWp0NBQ7dy5M0/7999/r+Dg4CsuCgAAAABcWZGCVO/evTV8+HCtXbtW2dnZys7O1po1azRixAj17t27uGsEAAAAAJdSpFn7XnrpJR0+fFht2rSRu/uFXeTk5Khfv348IwUAAADgmlekIOXp6akFCxboxRdf1Pfffy8fHx/Vr19fVapUKe76AAAAAMDlFClI5apVq5Zq1apVXLUAAAAAwFWhSEEqOztbs2bN0urVq3Xy5Enl5OQ4rF+zZk2xFAcAAAAArqhIQWrEiBGaNWuWOnbsqKioKNlstuKuCwAAAABcVpGC1Pz58/Xpp5+qQ4cOxV0PAAAAALi8Ik1/7unpqRo1ahR3LQAAAABwVShSkBo5cqSmTp0qy7KKux4AAAAAcHlFurVv48aNWrt2rf773/+qXr168vDwcFi/ePHiYikOAAAAAFxRkYJUuXLl1K1bt+KuBQAAAACuCkUKUjNnzizuOgAAAADgqlGkZ6Qk6fz581q1apXeffddpaenS5KOHz+u06dPF1txAAAAAOCKinRF6vDhw7r77rt15MgRZWRkKCYmRv7+/po4caL+/vtvvfPOO8VdJwAAAAC4jCJdkRoxYoSaNm2qU6dOycfHx97erVs3rV69utiKAwAAAABXVORZ+77++mt5eno6tFepUkW//vprsRQGAAAAAK6qSFekcnJylJ2dnaf92LFj8vf3v+KiAAAAAMCVFSlIxcTEaMqUKfbXNptNp0+f1tixY9WhQ4fiqg0AAAAAXFKRbu2bPHmyWrdurZtuukl///23+vTpo/379yskJETz5s0r7hoBAAAAwKUUKUhFREQoKSlJ8+bN0/bt25WTk6OBAwfq/vvvd5h8AgAAAACuRUUKUpLk4+Ojhx56SA899FBx1gMAAAAALq9IQeqjjz665Pp+/foVqRgAAAAAuBoUKUiNGDHC4XVWVpbOnj0rT09P+fr6EqQAAAAAXNOKNGvfqVOnHJbTp09r3759atmyJZNNAAAAALjmFSlI5admzZp65ZVX8lytAgAAAIBrTbEFKUlyc3PT8ePHi3OXAAAAAOByivSM1JIlSxxeW5al5ORkTZs2TbfffnuxFAYAAAAArqpIQeree+91eG2z2VShQgXdddddev3114ujLgAAAABwWUUKUjk5OcVdBwAAAABcNYr1GSkAAAAAuB4U6YrU448/Xui+kyZNKsohAAAAAMBlFSlI7dixQ9u3b9f58+dVu3ZtSdJPP/0kNzc3NW7c2N7PZrMVT5UAAAAA4EKKFKQ6d+4sf39/zZ49W+XLl5d04Ut6BwwYoDvuuEMjR44s1iIBAAAAwJXYLMuyTDe64YYbtHLlStWrV8+hfffu3WrXrt1V911SaWlpCgwMVGpqqgICApxdDgDASTp3dnYFjpYudXYFAHD9KWw2KNJkE2lpaTpx4kSe9pMnTyo9Pb0ouwQAAACAq0aRglS3bt00YMAALVq0SMeOHdOxY8e0aNEiDRw4UN27dy/uGgEAAADApRTpGal33nlHo0aN0gMPPKCsrKwLO3J318CBA/Xqq68Wa4EAAAAA4GqK9IxUrjNnzujAgQOyLEs1atSQn59fcdZWanhGCgAg8YwUAKCEn5HKlZycrOTkZNWqVUt+fn66gkwGAAAAAFeNIgWpP/74Q23atFGtWrXUoUMHJScnS5IGDRrE1OcAAAAArnlFClKPPfaYPDw8dOTIEfn6+trbe/XqpRUrVhRbcQAAAADgioo02cTKlSv1v//9T5UqVXJor1mzpg4fPlwshQEAAACAqyrSFakzZ844XInK9fvvv8vLy+uKiwIAAAAAV1akIHXnnXfqo48+sr+22WzKycnRq6++qtatWxdbcQAAAADgiop0a9+rr76q6Ohobd26VZmZmRo9erT27NmjP//8U19//XVx1wgAAAAALqVIV6Ruuukm7dy5U7feeqtiYmJ05swZde/eXTt27FD16tWLu0YAAAAAcCnGV6SysrLUrl07vfvuuxo3blxJ1AQAAAAALs34ipSHh4d2794tm81WEvUAAAAAgMsr0q19/fr104wZM4q7FgAAAAC4KhRpsonMzEx98MEHSkxMVNOmTeXn5+ewftKkScVSHAAAAAC4IqMg9csvv6hq1aravXu3GjduLEn66aefHPpwyx8AAACAa51RkKpZs6aSk5O1du1aSVKvXr30xhtvKCwsrESKAwAAAABXZPSMlGVZDq//+9//6syZM8VaEAAAAAC4uiJNNpHr4mAFAAAAANcDoyBls9nyPAPFM1EAAAAArjdGz0hZlqX+/fvLy8tLkvT333/rkUceyTNr3+LFi4uvQgAAAABwMUZB6sEHH3R4/cADDxRrMQAAAABwNTAKUjNnziypOgAAAADgqnFFk00AAAAAwPWIIAUAAAAAhghSAAAAAGCIIAUAAAAAhghSAAAAAGCIIAUAAAAAhghSAAAAAGCIIAUAAAAAhghSAAAAAGCIIAUAAAAAhghSAAAAAGCIIAUAAAAAhghSAAAAAGDIqUEqPj5et9xyi/z9/RUaGqp7771X+/btc+hjWZbi4uIUEREhHx8fRUdHa8+ePQ59MjIyNGzYMIWEhMjPz09dunTRsWPHSvNUAAAAAFxHnBqk1q9fr6FDh+qbb75RYmKizp8/r3bt2unMmTP2PhMnTtSkSZM0bdo0bdmyReHh4YqJiVF6erq9T2xsrBISEjR//nxt3LhRp0+fVqdOnZSdne2M0wIAAABwjbNZlmU5u4hcv/32m0JDQ7V+/XrdeeedsixLERERio2N1ZNPPinpwtWnsLAwTZgwQYMHD1ZqaqoqVKigjz/+WL169ZIkHT9+XJGRkVq+fLnat29/2eOmpaUpMDBQqampCggIKNFzBAC4rs6dnV2Bo6VLnV0BAFx/CpsNXOoZqdTUVElSUFCQJOngwYNKSUlRu3bt7H28vLzUqlUrbdq0SZK0bds2ZWVlOfSJiIhQVFSUvc/FMjIylJaW5rAAAAAAQGG5TJCyLEuPP/64WrZsqaioKElSSkqKJCksLMyhb1hYmH1dSkqKPD09Vb58+QL7XCw+Pl6BgYH2JTIysrhPBwAAAMA1zGWC1KOPPqqdO3dq3rx5edbZbDaH15Zl5Wm72KX6PP3000pNTbUvR48eLXrhAAAAAK47LhGkhg0bpiVLlmjt2rWqVKmSvT08PFyS8lxZOnnypP0qVXh4uDIzM3Xq1KkC+1zMy8tLAQEBDgsAAAAAFJZTg5RlWXr00Ue1ePFirVmzRtWqVXNYX61aNYWHhysxMdHelpmZqfXr16tFixaSpCZNmsjDw8OhT3Jysnbv3m3vAwAAAADFyd2ZBx86dKg++eQTffHFF/L397dfeQoMDJSPj49sNptiY2M1fvx41axZUzVr1tT48ePl6+urPn362PsOHDhQI0eOVHBwsIKCgjRq1CjVr19fbdu2debpAQAAALhGOTVITZ8+XZIUHR3t0D5z5kz1799fkjR69GidO3dOQ4YM0alTp9SsWTOtXLlS/v7+9v6TJ0+Wu7u7evbsqXPnzqlNmzaaNWuW3NzcSutUAAAAAFxHXOp7pJyF75ECAEh8jxQA4Cr9HikAAAAAuBoQpAAAAADAEEEKAAAAAAwRpAAAAADAEEEKAAAAAAwRpAAAAADAEEEKAAAAAAwRpAAAAADAEEEKAAAAAAwRpAAAAADAEEEKAAAAAAwRpAAAAADAEEEKAAAAAAwRpAAAAADAEEEKAAAAAAwRpAAAAADAEEEKAAAAAAwRpAAAAADAEEEKAAAAAAwRpAAAAADAEEEKAAAAAAwRpAAAAADAEEEKAAAAAAwRpAAAAADAEEEKAAAAAAwRpAAAAADAEEEKAAAAAAwRpAAAAADAEEEKAAAAAAwRpAAAAADAEEEKAAAAAAwRpAAAAADAEEEKAAAAAAwRpAAAAADAEEEKAAAAAAwRpAAAAADAEEEKAAAAAAwRpAAAAADAEEEKAAAAAAwRpAAAAADAEEEKAAAAAAwRpAAAAADAEEEKAAAAAAwRpAAAAADAEEEKAAAAAAwRpAAAAADAEEEKAAAAAAwRpAAAAADAEEEKAAAAAAwRpAAAAADAEEEKAAAAAAwRpAAAAADAEEEKAAAAAAwRpAAAAADAEEEKAAAAAAwRpAAAAADAEEEKAAAAAAwRpAAAAADAEEEKAAAAAAwRpAAAAADAEEEKAAAAAAwRpAAAAADAEEEKAAAAAAwRpAAAAADAEEEKAAAAAAwRpAAAAADAEEEKAAAAAAwRpAAAAADAEEEKAAAAAAwRpAAAAADAEEEKAAAAAAwRpAAAAADAEEEKAAAAAAwRpAAAAADAEEEKAAAAAAwRpAAAAADAEEEKAAAAAAw5NUh99dVX6ty5syIiImSz2fT55587rLcsS3FxcYqIiJCPj4+io6O1Z88ehz4ZGRkaNmyYQkJC5Ofnpy5duujYsWOleBYAAAAArjdODVJnzpxRw4YNNW3atHzXT5w4UZMmTdK0adO0ZcsWhYeHKyYmRunp6fY+sbGxSkhI0Pz587Vx40adPn1anTp1UnZ2dmmdBgAAAIDrjM2yLMvZRUiSzWZTQkKC7r33XkkXrkZFREQoNjZWTz75pKQLV5/CwsI0YcIEDR48WKmpqapQoYI+/vhj9erVS5J0/PhxRUZGavny5Wrfvn2hjp2WlqbAwEClpqYqICCgRM4PAOD6Ond2dgWOli51dgUAcP0pbDZw2WekDh48qJSUFLVr187e5uXlpVatWmnTpk2SpG3btikrK8uhT0REhKKioux98pORkaG0tDSHBQAAAAAKy2WDVEpKiiQpLCzMoT0sLMy+LiUlRZ6enipfvnyBffITHx+vwMBA+xIZGVnM1QMAAAC4lrlskMpls9kcXluWlaftYpfr8/TTTys1NdW+HD16tFhqBQAAAHB9cNkgFR4eLkl5riydPHnSfpUqPDxcmZmZOnXqVIF98uPl5aWAgACHBQAAAAAKy2WDVLVq1RQeHq7ExER7W2ZmptavX68WLVpIkpo0aSIPDw+HPsnJydq9e7e9DwAAAAAUN3dnHvz06dP6+eef7a8PHjyopKQkBQUFqXLlyoqNjdX48eNVs2ZN1axZU+PHj5evr6/69OkjSQoMDNTAgQM1cuRIBQcHKygoSKNGjVL9+vXVtm1bZ50WAAAAgGucU4PU1q1b1bp1a/vrxx9/XJL04IMPatasWRo9erTOnTunIUOG6NSpU2rWrJlWrlwpf39/+zaTJ0+Wu7u7evbsqXPnzqlNmzaaNWuW3NzcSv18AAAAAFwfXOZ7pJyJ75ECAEh8jxQA4Br4HikAAAAAcFUEKQAAAAAwRJACAAAAAEMEKQAAAAAwRJACAAAAAEMEKQAAAAAwRJACAAAAAEMEKQAAAAAwRJACAAAAAEMEKQAAAAAwRJACAAAAAEMEKQAAAAAwRJACAAAAAEMEKQAAAAAwRJACAAAAAEMEKQAAAAAwRJACAAAAAEMEKQAAAAAwRJACAAAAAEMEKQAAAAAwRJACAAAAAEMEKQAAAAAwRJACAAAAAEMEKQAAAAAwRJACAAAAAEMEKQAAAAAwRJACAAAAAEMEKQAAAAAwRJACAAAAAEMEKQAAAAAwRJACAAAAAEMEKQAAAAAwRJACAAAAAEMEKQAAAAAwRJACAAAAAEMEKQAAAAAwRJACAAAAAEMEKQAAAAAwRJACAAAAAEMEKQAAAAAwRJACAAAAAEMEKQAAAAAwRJACAAAAAEMEKQAAAAAwRJACAAAAAEMEKQAAAAAwRJACAAAAAEMEKQAAAAAwRJACAAAAAEMEKQAAAAAwRJACAAAAAEMEKQAAAAAwRJACAAAAAEMEKQAAAAAwRJACAAAAAEMEKQAAAAAwRJACAAAAAEMEKQAAAAAwRJACAAAAAEMEKQAAAAAwRJACAAAAAEMEKQAAAAAwRJACAAAAAEMEKQAAAAAwRJACAAAAAEMEKQAAAAAwRJACAAAAAEMEKQAAAAAwRJACAAAAAEMEKQAAAAAwRJACAAAAAEMEKQAAAAAwRJACAAAAAEMEKQAAAAAwRJACAAAAAEMEKQAAAAAwRJACAAAAAEPXTJB6++23Va1aNXl7e6tJkybasGGDs0sCAAAAcI26JoLUggULFBsbqzFjxmjHjh264447dM899+jIkSPOLg0AAADANeiaCFKTJk3SwIEDNWjQINWtW1dTpkxRZGSkpk+f7uzSAAAAAFyD3J1dwJXKzMzUtm3b9NRTTzm0t2vXTps2bcp3m4yMDGVkZNhfp6amSpLS0tJKrlAAgMvLynJ2BY74bwkASl9uJrAs65L9rvog9fvvvys7O1thYWEO7WFhYUpJScl3m/j4eI0bNy5Pe2RkZInUCABAUQQGOrsCALh+paenK/AS/xBf9UEql81mc3htWVaetlxPP/20Hn/8cfvrnJwc/fnnnwoODi5wGzhfWlqaIiMjdfToUQUEBDi7HLg4xgtMMWZgijEDU4yZq4NlWUpPT1dERMQl+131QSokJERubm55rj6dPHkyz1WqXF5eXvLy8nJoK1euXEmViGIWEBDAPz4oNMYLTDFmYIoxA1OMGdd3qStRua76ySY8PT3VpEkTJSYmOrQnJiaqRYsWTqoKAAAAwLXsqr8iJUmPP/64+vbtq6ZNm6p58+Z67733dOTIET3yyCPOLg0AAADANeiaCFK9evXSH3/8oRdeeEHJycmKiorS8uXLVaVKFWeXhmLk5eWlsWPH5rktE8gP4wWmGDMwxZiBKcbMtcVmXW5ePwAAAACAg6v+GSkAAAAAKG0EKQAAAAAwRJACAAAAAEMEKQAAAAAwRJCCyzp16pT69u2rwMBABQYGqm/fvvrrr78Kvf3gwYNls9k0ZcqUEqsRrsV0zGRlZenJJ59U/fr15efnp4iICPXr10/Hjx8vvaJRqt5++21Vq1ZN3t7eatKkiTZs2HDJ/uvXr1eTJk3k7e2tG2+8Ue+8804pVQpXYTJmFi9erJiYGFWoUEEBAQFq3ry5/ve//5VitXAFpv/O5Pr666/l7u6uRo0alWyBKDYEKbisPn36KCkpSStWrNCKFSuUlJSkvn37Fmrbzz//XN9++60iIiJKuEq4EtMxc/bsWW3fvl3PPfectm/frsWLF+unn35Sly5dSrFqlJYFCxYoNjZWY8aM0Y4dO3THHXfonnvu0ZEjR/Ltf/DgQXXo0EF33HGHduzYoWeeeUbDhw/XZ599VsqVw1lMx8xXX32lmJgYLV++XNu2bVPr1q3VuXNn7dixo5Qrh7OYjplcqamp6tevn9q0aVNKlaJYWIAL+uGHHyxJ1jfffGNv27x5syXJ+vHHHy+57bFjx6wbbrjB2r17t1WlShVr8uTJJVwtXMGVjJl/+u677yxJ1uHDh0uiTDjRrbfeaj3yyCMObXXq1LGeeuqpfPuPHj3aqlOnjkPb4MGDrdtuu63EaoRrMR0z+bnpppuscePGFXdpcFFFHTO9evWynn32WWvs2LFWw4YNS7BCFCeuSMElbd68WYGBgWrWrJm97bbbblNgYKA2bdpU4HY5OTnq27evnnjiCdWrV680SoWLKOqYuVhqaqpsNpvKlStXAlXCWTIzM7Vt2za1a9fOob1du3YFjo/Nmzfn6d++fXtt3bpVWVlZJVYrXENRxszFcnJylJ6erqCgoJIoES6mqGNm5syZOnDggMaOHVvSJaKYuTu7ACA/KSkpCg0NzdMeGhqqlJSUArebMGGC3N3dNXz48JIsDy6oqGPmn/7++2899dRT6tOnjwICAoq7RDjR77//ruzsbIWFhTm0h4WFFTg+UlJS8u1//vx5/f7776pYsWKJ1QvnK8qYudjrr7+uM2fOqGfPniVRIlxMUcbM/v379dRTT2nDhg1yd+fX8qsNV6RQquLi4mSz2S65bN26VZJks9nybG9ZVr7tkrRt2zZNnTpVs2bNKrAPrj4lOWb+KSsrS71791ZOTo7efvvtYj8PuIaLx8Llxkd+/fNrx7XLdMzkmjdvnuLi4rRgwYJ8/8iDa1dhx0x2drb69OmjcePGqVatWqVVHooR0Rel6tFHH1Xv3r0v2adq1arauXOnTpw4kWfdb7/9lucvPbk2bNigkydPqnLlyva27OxsjRw5UlOmTNGhQ4euqHY4R0mOmVxZWVnq2bOnDh48qDVr1nA16hoUEhIiNze3PH8VPnnyZIHjIzw8PN/+7u7uCg4OLrFa4RqKMmZyLViwQAMHDtTChQvVtm3bkiwTLsR0zKSnp2vr1q3asWOHHn30UUkXbge1LEvu7u5auXKl7rrrrlKpHUVDkEKpCgkJUUhIyGX7NW/eXKmpqfruu+906623SpK+/fZbpaamqkWLFvlu07dv3zz/YbVv3159+/bVgAEDrrx4OEVJjhnp/4Wo/fv3a+3atfyCfI3y9PRUkyZNlJiYqG7dutnbExMT1bVr13y3ad68uZYuXerQtnLlSjVt2lQeHh4lWi+cryhjRrpwJeqhhx7SvHnz1LFjx9IoFS7CdMwEBARo165dDm1vv/221qxZo0WLFqlatWolXjOukBMnugAu6e6777YaNGhgbd682dq8ebNVv359q1OnTg59ateubS1evLjAfTBr3/XFdMxkZWVZXbp0sSpVqmQlJSVZycnJ9iUjI8MZp4ASNH/+fMvDw8OaMWOG9cMPP1ixsbGWn5+fdejQIcuyLOupp56y+vbta+//yy+/WL6+vtZjjz1m/fDDD9aMGTMsDw8Pa9GiRc46BZQy0zHzySefWO7u7tZbb73l8O/JX3/95axTQCkzHTMXY9a+qwtXpOCy5s6dq+HDh9tnv+nSpYumTZvm0Gffvn1KTU11RnlwQaZj5tixY1qyZIkk5fkCxLVr1yo6OrrEa0bp6dWrl/744w+98MILSk5OVlRUlJYvX64qVapIkpKTkx2+66VatWpavny5HnvsMb311luKiIjQG2+8oR49ejjrFFDKTMfMu+++q/Pnz2vo0KEaOnSovf3BBx/UrFmzSrt8OIHpmMHVzWZZ//+TswAAAACAQmHWPgAAAAAwRJACAAAAAEMEKQAAAAAwRJACAAAAAEMEKQAAAAAwRJACAAAAAEMEKQAAAAAwRJACAAAAcFWIjo6WzWZzWHr37m20j8WLF6tp06YqV66c/Pz81KhRI3388cfGtRCkAAAur3///rr33nuLfb8pKSmKiYmRn5+fypUrV6rHLglVq1bVlClTLtnHZrPp888/L5V6AKAooqOjNWvWrALX/9///Z+Sk5Pty7vvvmu0/6CgII0ZM0abN2/Wzp07NWDAAA0YMED/+9//jPZDkAIASHKNwHDo0CHZbDYlJSWVyvEmT56s5ORkJSUl6aeffsq3z9SpUy/5H3pJmTVrVoHhriBbtmzRww8/XDIFAYCL8PX1VXh4uH0JDAx0WP/rr7+qV69eKl++vIKDg9W1a1cdOnTIvj46OlrdunVT3bp1Vb16dY0YMUINGjTQxo0bjeogSAEArlsHDhxQkyZNVLNmTYWGhubbJzAw0DjQOEuFChXk6+vr7DIAoETNnTtXISEhqlevnkaNGqX09HT7urNnz6p169YqW7asvvrqK23cuFFly5bV3XffrczMzDz7sixLq1ev1r59+3TnnXca1UGQAgAUyg8//KAOHTqobNmyCgsLU9++ffX777/b10dHR2v48OEaPXq0goKCFB4erri4OId9/Pjjj2rZsqW8vb110003adWqVQ63mlWrVk2SdPPNN8tmsyk6Otph+9dee00VK1ZUcHCwhg4dqqysrEvWPH36dFWvXl2enp6qXbu2wz3wVatW1WeffaaPPvpINptN/fv3z3cfF1+pK8x52mw2TZ8+Xffcc498fHxUrVo1LVy40L5+3bp1stls+uuvv+xtSUlJstlsOnTokNatW6cBAwYoNTXV/gzAxcfIz8W39u3fv1933nmn/f1OTEx06J+ZmalHH31UFStWlLe3t6pWrar4+PjLHgcAnOX+++/XvHnztG7dOj333HP67LPP1L17d/v6+fPnq0yZMvrggw9Uv3591a1bVzNnztSRI0e0bt06e7/U1FSVLVtWnp6e6tixo958803FxMQY1UKQAgBcVnJyslq1aqVGjRpp69atWrFihU6cOKGePXs69Js9e7b8/Pz07bffauLEiXrhhRfsv7zn5OTo3nvvla+vr7799lu99957GjNmjMP23333nSRp1apVSk5O1uLFi+3r1q5dqwMHDmjt2rWaPXu2Zs2adclb7hISEjRixAiNHDlSu3fv1uDBgzVgwACtXbtW0oXb4O6++2717NlTycnJmjp1aqHfj0udZ67nnntOPXr00Pfff68HHnhA//73v7V3795C7b9FixaaMmWKAgIC7M8AjBo1qtD1SRfe7+7du8vNzU3ffPON3nnnHT355JMOfd544w0tWbJEn376qfbt26c5c+aoatWqRscBgCs1fvx4lS1b1r5s2LBBjzzySJ426cLzUW3btlVUVJR69+6tRYsWadWqVdq+fbskadu2bfr555/l7+9v3zYoKEh///23Dhw4YD+mv7+/kpKStGXLFr388st6/PHHHYJWYbgX2zsAALhmTZ8+XY0bN9b48ePtbR9++KEiIyP1008/qVatWpKkBg0aaOzYsZKkmjVratq0aVq9erViYmK0cuVKHThwQOvWrVN4eLgk6eWXX3b4C2CFChUkScHBwfY+ucqXL69p06bJzc1NderUUceOHbV69Wr93//9X741v/baa+rfv7+GDBkiSXr88cf1zTff6LXXXlPr1q1VoUIFeXl5ycfHJ8+xLudS55nrX//6lwYNGiRJevHFF5WYmKg333xTb7/99mX37+npqcDAQNlsNuPacq1atUp79+7VoUOHVKlSJUkXflm555577H2OHDmimjVrqmXLlrLZbKpSpUqRjgUAV+KRRx5x+MPc/fffrx49ejhcabrhhhvy3bZx48by8PDQ/v371bhxY+Xk5KhJkyaaO3dunr65/8dIUpkyZVSjRg1JUqNGjbR3717Fx8fnuRPiUghSAIDL2rZtm9auXauyZcvmWXfgwAGHIPVPFStW1MmTJyVJ+/btU2RkpEMwuPXWWwtdQ7169eTm5uaw7127dhXYf+/evXkmXrj99tuNrjwV5FLnmat58+Z5XpfWJBrShfOvXLmyPUTlV1P//v0VExOj2rVr6+6771anTp3Url27UqsRAKQLs+gFBQXZX/v4+Cg0NNQedC5lz549ysrKUsWKFSVdCFYLFixQaGioAgICCl2DZVnKyMgwqptb+wAAl5WTk6POnTsrKSnJYcl9BieXh4eHw3Y2m005OTmSLvwnZbPZilzDpfZdkIuPd6U1XEkt/6ynTJky9npyXe55L1P/3PfFx8/VuHFjHTx4UC+++KLOnTunnj176r777ivWOgCguBw4cEAvvPCCtm7dqkOHDmn58uX617/+pZtvvlm33367pAtXs0JCQtS1a1dt2LBBBw8e1Pr16zVixAgdO3ZMkhQfH6/ExET98ssv+vHHHzVp0iR99NFHeuCBB4zqIUgBAC6rcePG2rNnj6pWraoaNWo4LH5+foXaR506dXTkyBGdOHHC3rZlyxaHPp6enpKk7OzsK665bt26eaay3bRpk+rWrXvF+y6Mb775Js/rOnXqSPp/t5ckJyfb1198tcrT0/OK3oebbrpJR44c0fHjx+1tmzdvztMvICBAvXr10vvvv68FCxbos88+059//lnk4wJASfH09NTq1avVvn171a5dW8OHD1e7du20atUq+x0Lvr6++uqrr1S5cmV1795ddevW1UMPPaRz587Zr1CdOXNGQ4YMUb169dSiRQstWrRIc+bMsd+OXVjc2gcAsEtNTc3zC31QUJCGDh2q999/X//+97/1xBNPKCQkRD///LPmz5+v999/3+GWu4LExMSoevXqevDBBzVx4kSlp6fbJ5vIvVISGhoqHx8frVixQpUqVZK3t3ee7wcprCeeeEI9e/ZU48aN1aZNGy1dulSLFy/WqlWrirQ/UwsXLlTTpk3VsmVLzZ07V999951mzJghSapRo4YiIyMVFxenl156Sfv379frr7/usH3VqlV1+vRprV69Wg0bNpSvr6/R1OZt27ZV7dq11a9fP73++utKS0vLM7nH5MmTVbFiRTVq1EhlypTRwoULFR4eftVM9w7g2lTQpA+RkZFav379ZbcPDw/X7NmzC1z/0ksv6aWXXipqeXZckQIA2K1bt04333yzw/L8888rIiJCX3/9tbKzs9W+fXtFRUVpxIgRCgwMtN+mdjlubm76/PPPdfr0ad1yyy0aNGiQnn32WUmSt7e3JMnd3V1vvPGG3n33XUVERKhr165FPpd7771XU6dO1auvvqp69erp3Xff1cyZM40eJL4S48aN0/z589WgQQPNnj1bc+fO1U033STpwq2B8+bN048//qiGDRtqwoQJef5Tb9GihR555BH16tVLFSpU0MSJE42OX6ZMGSUkJCgjI0O33nqrBg0apJdfftmhT9myZTVhwgQ1bdpUt9xyi/1WmcJ+pgBwPbNZ+d1EDQBAKfj666/VsmVL/fzzz6pevbqzyyk2NptNCQkJDt8/BQC4tnBrHwCg1CQkJKhs2bKqWbOmfv75Z40YMUK33377NRWiAADXB4IUAKDUpKena/To0Tp69KhCQkLUtm3bPM8GIX8bNmxw+A6oi50+fboUqwEAcGsfAABXgXPnzunXX38tcH1hvm8FAFB8CFIAAAAAYIhpeQAAAADAEEEKAAAAAAwRpAAAAADAEEEKAAAAAAwRpAAAAADAEEEKAAAAAAwRpAAAAADA0P8HunmZF0mavWoAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jP3R4enP3m19"
   },
   "source": [
    "### How does the base model do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vxbl4ACsyRgi"
   },
   "source": [
    "Optionally, you can check how Mistral does on one of your data samples. For example, if you have a dataset of users' biometric data to their health scores, you could test the following `eval_prompt`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "gOxnx-cAyRgi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Question:\n",
      "You are a Student Web Activity Analyzer developed to support professionals, including Social Workers, School Psychologists, District Administrators, School Safety Specialists, and related roles. Your primary objective is to meticulously evaluate the online activity of K-12 students and identify specific indicators related to their interests and passions. For each identified indicator, provide a JSON object containing:\n",
      "\n",
      "Presence: Indicate a value of 1 (if the indicator is present) or 0 (if not). Even if only a portion of the data aligns with an indicator, mark it as 1.\n",
      "Confidence: Assign a confidence level on a scale of 1-10 to indicate the certainty level of your analysis.\n",
      "\n",
      "Additionally, please include a note that outlines the rationale behind identifying certain indicators and offers a summary of the analyzed web activity.\n",
      "\n",
      "Adhere to the JSON format outlined in the example output section precisely.\n",
      "\n",
      "Each individual online activity you receive represents one search or interaction on a student's device. Occasionally, searches that include large amounts of text will be summarized. These summaries will be marked with 'S~'. Such summarization typically occurs when students copy and paste extensive text blocks, although other cases may exist. Additional details will be provided in the summary.\n",
      "\n",
      "In situations where the presence of an indicator is ambiguous or if anomalies are present in the data, exercise your best judgment while providing a confidence level that reflects the level of uncertainty.\n",
      "\n",
      "Here are the specific indicators that you should use for this task, with definitions, delimited by single quotes.\n",
      "\n",
      "'sports-and-athletics: participating in physical activities and team sports to promote fitness, teamwork, and sportsmanship.'\n",
      "'environmentalism-and-sustainability: learning about the environment, conservation, and sustainable practices to become responsible global citizens.'\n",
      "'gaming-and-e-sports: engaging in digital gaming and competitive e-sports to develop strategic thinking, problem-solving, and teamwork skills.'\n",
      "'college-and-career: engaging in planning, research, and/or discovery around future college and career opportunities or otherwise demonstrating an interest in college or career activities after high school'\n",
      "'cooking-and-food: investigating cooking or food'\n",
      "'reading-and-literature: exploring the world of books and stories through reading and interpretation.'\n",
      "'writing-and-creative-writing: expressing thoughts, ideas, and imagination through written words and storytelling.'\n",
      "'science-and-technology: investigating the natural world and technological advancements'\n",
      "'mathematics-and-statistics: engaging in problem-solving and numerical analysis to understand patterns, shapes, and quantities.'\n",
      "'history-and-social-studies: discovering past events, cultures, and societies to gain a deeper understanding of the world.'\n",
      "'creative-arts: expressing creativity through various art forms like drawing, painting, sculpture, music, performing arts, and more'\n",
      "'animals-and-nature: reflects a student's enthusiasm and curiosity for studying, observing, or interacting with animals and natural environments, potentially driving academic pursuits, extracurricular activities, or career paths related to biology, ecology, or conservation.'\n",
      "\n",
      "\n",
      "### Search Data:\n",
      "['alexs', 'slope formula', 'aslope']\n",
      "\n",
      "### Example Output:\n",
      "{\n",
      "  \"sports-and-athletics\": \"1\",\n",
      "  \"sports-and-athletics-confidence\": \"6\",\n",
      "  \"environmentalism-and-sustainability\": \"0\",\n",
      "  \"environmentalism-and-sustainability-confidence\": \"8\",\n",
      "  \"gaming-and-e-sports\": \"0\",\n",
      "  \"gaming-and-e-sports-confidence\": \"10\",\n",
      "  \"college-and-career\": \"1\",\n",
      "  \"college-and-career-confidence\": \"7\",\n",
      "  \"cooking-and-food\": \"1\",\n",
      "  \"cooking-and-food-confidence\": \"7\",\n",
      "  \"reading-and-literature\": \"0\",\n",
      "  \"reading-and-literature-confidence\": \"7\",\n",
      "  \"writing-and-creative-writing\": \"1\",\n",
      "  \"writing-and-creative-writing-confidence\": \"8\",\n",
      "  \"science-and-technology\": \"0\",\n",
      "  \"science-and-technology-confidence\": \"10\",\n",
      "  \"mathematics-and-statistics\": \"1\",\n",
      "  \"mathematics-and-statistics-confidence\": \"6\",\n",
      "  \"history-and-social-studies\": \"9\",\n",
      "  \"history-and-social-studies-confidence\": \"0\",\n",
      "  \"creative-arts\": \"1\",\n",
      "  \"creative-arts-confidence\": \"8\",\n",
      "  \"animals-and-nature\": \"1\",\n",
      "  \"animals-and-nature-confidence\": \"9\",\n",
      "  \"note\": \"Detailed Summary Goes Here \"\n",
      "}\n",
      "\n",
      "### Solution:\n"
     ]
    }
   ],
   "source": [
    "eval_prompt = question_func(data.loc[0])\n",
    "print(eval_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "NidIuFXMyRgi",
    "outputId": "b1794b11-9a22-4b0a-e871-7df039ab59fc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Question:\n",
      "You are a Student Web Activity Analyzer developed to support professionals, including Social Workers, School Psychologists, District Administrators, School Safety Specialists, and related roles. Your primary objective is to meticulously evaluate the online activity of K-12 students and identify specific indicators related to their interests and passions. For each identified indicator, provide a JSON object containing:\n",
      "\n",
      "Presence: Indicate a value of 1 (if the indicator is present) or 0 (if not). Even if only a portion of the data aligns with an indicator, mark it as 1.\n",
      "Confidence: Assign a confidence level on a scale of 1-10 to indicate the certainty level of your analysis.\n",
      "\n",
      "Additionally, please include a note that outlines the rationale behind identifying certain indicators and offers a summary of the analyzed web activity.\n",
      "\n",
      "Adhere to the JSON format outlined in the example output section precisely.\n",
      "\n",
      "Each individual online activity you receive represents one search or interaction on a student's device. Occasionally, searches that include large amounts of text will be summarized. These summaries will be marked with 'S~'. Such summarization typically occurs when students copy and paste extensive text blocks, although other cases may exist. Additional details will be provided in the summary.\n",
      "\n",
      "In situations where the presence of an indicator is ambiguous or if anomalies are present in the data, exercise your best judgment while providing a confidence level that reflects the level of uncertainty.\n",
      "\n",
      "Here are the specific indicators that you should use for this task, with definitions, delimited by single quotes.\n",
      "\n",
      "'sports-and-athletics: participating in physical activities and team sports to promote fitness, teamwork, and sportsmanship.'\n",
      "'environmentalism-and-sustainability: learning about the environment, conservation, and sustainable practices to become responsible global citizens.'\n",
      "'gaming-and-e-sports: engaging in digital gaming and competitive e-sports to develop strategic thinking, problem-solving, and teamwork skills.'\n",
      "'college-and-career: engaging in planning, research, and/or discovery around future college and career opportunities or otherwise demonstrating an interest in college or career activities after high school'\n",
      "'cooking-and-food: investigating cooking or food'\n",
      "'reading-and-literature: exploring the world of books and stories through reading and interpretation.'\n",
      "'writing-and-creative-writing: expressing thoughts, ideas, and imagination through written words and storytelling.'\n",
      "'science-and-technology: investigating the natural world and technological advancements'\n",
      "'mathematics-and-statistics: engaging in problem-solving and numerical analysis to understand patterns, shapes, and quantities.'\n",
      "'history-and-social-studies: discovering past events, cultures, and societies to gain a deeper understanding of the world.'\n",
      "'creative-arts: expressing creativity through various art forms like drawing, painting, sculpture, music, performing arts, and more'\n",
      "'animals-and-nature: reflects a student's enthusiasm and curiosity for studying, observing, or interacting with animals and natural environments, potentially driving academic pursuits, extracurricular activities, or career paths related to biology, ecology, or conservation.'\n",
      "\n",
      "\n",
      "### Search Data:\n",
      "['alexs', 'slope formula', 'aslope']\n",
      "\n",
      "### Example Output:\n",
      "{\n",
      "  \"sports-and-athletics\": \"1\",\n",
      "  \"sports-and-athletics-confidence\": \"6\",\n",
      "  \"environmentalism-and-sustainability\": \"0\",\n",
      "  \"environmentalism-and-sustainability-confidence\": \"8\",\n",
      "  \"gaming-and-e-sports\": \"0\",\n",
      "  \"gaming-and-e-sports-confidence\": \"10\",\n",
      "  \"college-and-career\": \"1\",\n",
      "  \"college-and-career-confidence\": \"7\",\n",
      "  \"cooking-and-food\": \"1\",\n",
      "  \"cooking-and-food-confidence\": \"7\",\n",
      "  \"reading-and-literature\": \"0\",\n",
      "  \"reading-and-literature-confidence\": \"7\",\n",
      "  \"writing-and-creative-writing\": \"1\",\n",
      "  \"writing-and-creative-writing-confidence\": \"8\",\n",
      "  \"science-and-technology\": \"0\",\n",
      "  \"science-and-technology-confidence\": \"10\",\n",
      "  \"mathematics-and-statistics\": \"1\",\n",
      "  \"mathematics-and-statistics-confidence\": \"6\",\n",
      "  \"history-and-social-studies\": \"9\",\n",
      "  \"history-and-social-studies-confidence\": \"0\",\n",
      "  \"creative-arts\": \"1\",\n",
      "  \"creative-arts-confidence\": \"8\",\n",
      "  \"animals-and-nature\": \"1\",\n",
      "  \"animals-and-nature-confidence\": \"9\",\n",
      "  \"note\": \"Detailed Summary Goes Here \"\n",
      "}\n",
      "\n",
      "### Solution:\n",
      "import json\n",
      "from collections import Counter\n",
      "\n",
      "def analyze_web_activity(search_data):\n",
      "    # Replace this code with your solution\n",
      "    # Initialize variables\n",
      "    indicators = {\n",
      "        'sports-and-athletics': 0,\n",
      "        'environmentalism-and-sustainability': 0,\n",
      "        'gaming-and-e-sports': 0,\n",
      "        'college-and-career': 0,\n",
      "        'cooking-and-food': 0,\n",
      "        'reading-and-literature': 0,\n",
      "        'writing-and-creative-writing': 0,\n",
      "        'science-and-technology': 0,\n",
      "        'mathematics-and-statistics': 0,\n",
      "        'history-and-social-studies': 0,\n",
      "        'creative-arts': 0,\n",
      "        'animals-and-nature': 0\n",
      "    }\n",
      "\n",
      "    # Loop over search data\n",
      "    for search in search_data:\n",
      "        # Check if any of the indicators match the search term\n",
      "        for indicator, count in Coun\n"
     ]
    }
   ],
   "source": [
    "# Re-init the tokenizer so it doesn't add padding or eos token\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    add_bos_token=True,\n",
    ")\n",
    "\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=256, repetition_penalty=1.15)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AapDoyfAyRgi"
   },
   "source": [
    "### 4. Set Up LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mp2gMi1ZzGET"
   },
   "source": [
    "Now, to start our fine-tuning, we have to apply some preprocessing to the model to prepare it for training. For that use the `prepare_model_for_kbit_training` method from PEFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "a9EUEDAl0ss3"
   },
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "gkIcwsSU01EB"
   },
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cUYEpEK-yRgj"
   },
   "source": [
    "Let's print the model to examine its layers, as we will apply QLoRA to all the linear layers of the model. Those layers are `q_proj`, `k_proj`, `v_proj`, `o_proj`, `gate_proj`, `up_proj`, `down_proj`, and `lm_head`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "XshGNsbxyRgj",
    "outputId": "c619b0e8-8516-4d4b-9abe-13eaa3f3b204",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm()\n",
      "        (post_attention_layernorm): MistralRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6mTLuQJyRgj"
   },
   "source": [
    "Here we define the LoRA config.\n",
    "\n",
    "`r` is the rank of the low-rank matrix used in the adapters, which thus controls the number of parameters trained. A higher rank will allow for more expressivity, but there is a compute tradeoff.\n",
    "\n",
    "`alpha` is the scaling factor for the learned weights. The weight matrix is scaled by `alpha/r`, and thus a higher value for `alpha` assigns more weight to the LoRA activations.\n",
    "\n",
    "The values used in the QLoRA paper were `r=64` and `lora_alpha=16`, and these are said to generalize well, but we will use `r=32` and `lora_alpha=64` so that we have more emphasis on the new fine-tuned data while also reducing computational complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "Ybeyl20n3dYH",
    "outputId": "6a16c182-04d9-4812-ae81-502a8fe364d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 85041152 || all params: 3837112320 || trainable%: 2.2162799758751914\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_FHi_VLyRgn"
   },
   "source": [
    "See how the model looks different now, with the LoRA adapters added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "IaYMWak4yRgn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): MistralForCausalLM(\n",
      "      (model): MistralModel(\n",
      "        (embed_tokens): Embedding(32000, 4096)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x MistralDecoderLayer(\n",
      "            (self_attn): MistralAttention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): MistralRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): MistralMLP(\n",
      "              (gate_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=14336, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): MistralRMSNorm()\n",
      "            (post_attention_layernorm): MistralRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): MistralRMSNorm()\n",
      "      )\n",
      "      (lm_head): lora.Linear(\n",
      "        (base_layer): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "        (lora_dropout): ModuleDict(\n",
      "          (default): Dropout(p=0.05, inplace=False)\n",
      "        )\n",
      "        (lora_A): ModuleDict(\n",
      "          (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "        )\n",
      "        (lora_B): ModuleDict(\n",
      "          (default): Linear(in_features=32, out_features=32000, bias=False)\n",
      "        )\n",
      "        (lora_embedding_A): ParameterDict()\n",
      "        (lora_embedding_B): ParameterDict()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05H5MIfjyRgc"
   },
   "source": [
    "### Accelerator\n",
    "\n",
    "Set up the Accelerator. I'm not sure if we really need this for a QLoRA given its [description](https://huggingface.co/docs/accelerate/v0.19.0/en/usage_guides/fsdp) (I have to read more about it) but it seems it can't hurt, and it's helpful to have the code for future reference. You can always comment out the accelerator if you want to try without."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "TEzYBadkyRgd"
   },
   "outputs": [],
   "source": [
    "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "\n",
    "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
    "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    ")\n",
    "\n",
    "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "yxSbpKQSLY6B"
   },
   "outputs": [],
   "source": [
    "model = accelerator.prepare_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9KNTJZkyRgn"
   },
   "source": [
    "\n",
    "Let's use Weights & Biases to track our training metrics. You'll need to apply an API key when prompted. Feel free to skip this if you'd like, and just comment out the `wandb` parameters in the `Trainer` definition below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "DDqUNyIoyRgo"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  Â·Â·Â·Â·Â·Â·Â·Â·\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    }
   ],
   "source": [
    "!pip install -q wandb -U\n",
    "\n",
    "import wandb, os\n",
    "wandb.login()\n",
    "\n",
    "wandb_project = \"discern-finetune\"\n",
    "if len(wandb_project) > 0:\n",
    "    os.environ[\"WANDB_PROJECT\"] = wandb_project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_0MOtwf3zdZp"
   },
   "source": [
    "### 5. Run Training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fEe0uWYSyRgo"
   },
   "source": [
    "I didn't have a lot of training samples: only about 200 total train/validation. I used 500 training steps, and I was fine with overfitting in this case. I found that the end product worked well. It took about 20 minutes on the 1x A10G 24GB.\n",
    "\n",
    "Overfitting is when the validation loss goes up (bad) while the training loss goes down significantly, meaning the model is learning the training set really well, but is unable to generalize to new datapoints. In most cases, this is not desired, but since I am just playing around with a model to generate outputs like my journal entries, I was fine with a moderate amount of overfitting.\n",
    "\n",
    "With that said, a note on training: you can set the `max_steps` to be high initially, and examine at what step your model's performance starts to degrade. There is where you'll find a sweet spot for how many steps to perform. For example, say you start with 1000 steps, and find that at around 500 steps the model starts overfitting, as described above. Therefore, 500 steps would be your sweet spot, so you would use the `checkpoint-500` model repo in your output dir (`mistral-journal-finetune`) as your final model in step 6 below.\n",
    "\n",
    "If you're just doing something for fun like I did and are OK with overfitting, you can try different checkpoint versions with different degrees of overfitting.\n",
    "\n",
    "You can interrupt the process via Kernel -> Interrupt Kernel in the top nav bar once you realize you didn't need to train anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "c_L1131GyRgo"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1: # If more than 1 GPU\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jq0nX33BmfaC",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdavid-hinkle\u001b[0m (\u001b[33msecurly\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/opt/notebooks/wandb/run-20231207_233514-acbgvx6u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/securly/discern-finetune/runs/acbgvx6u' target=\"_blank\">mistral-journal-finetune-2023-12-07-23-35</a></strong> to <a href='https://wandb.ai/securly/discern-finetune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/securly/discern-finetune' target=\"_blank\">https://wandb.ai/securly/discern-finetune</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/securly/discern-finetune/runs/acbgvx6u' target=\"_blank\">https://wandb.ai/securly/discern-finetune/runs/acbgvx6u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/opt/conda/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   2/1000 : < :, Epoch 0.01/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import transformers\n",
    "from datetime import datetime\n",
    "\n",
    "project = \"discern-finetune\"\n",
    "base_model_name = \"mistral\"\n",
    "run_name = base_model_name + \"-\" + project\n",
    "output_dir = \"./\" + run_name\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        warmup_steps=1,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=1,\n",
    "        gradient_checkpointing=True,\n",
    "        max_steps=1000,\n",
    "        learning_rate=2.5e-5, # Want a small lr for finetuning\n",
    "        bf16=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        logging_steps=25,              # When to start reporting loss\n",
    "        logging_dir=\"./logs\",        # Directory for storing logs\n",
    "        save_strategy=\"steps\",       # Save the model checkpoint every logging step\n",
    "        save_steps=25,                # Save checkpoints every 50 steps\n",
    "        evaluation_strategy=\"steps\", # Evaluate the model every logging step\n",
    "        eval_steps=25,               # Evaluate and save checkpoints every 50 steps\n",
    "        do_eval=True,                # Perform evaluation at the end of training\n",
    "        report_to=\"wandb\",           # Comment this out if you don't want to use weights & baises\n",
    "        run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"          # Name of the W&B run (optional)\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0D57XqcsyRgo"
   },
   "source": [
    "### 6. Drum Roll... Try the Trained Model!\n",
    "\n",
    "It's a good idea to kill the current process so that you don't run out of memory loading the base model again on top of the model we just trained. Go to `Kernel > Restart Kernel` or kill the process via the Terminal (`nvidia smi` > `kill [PID]`). \n",
    "\n",
    "By default, the PEFT library will only save the QLoRA adapters, so we need to first load the base model from the Huggingface Hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "fb8230fb86884aa6be318e2d03a88af2"
     ]
    },
    "id": "SKSnF016yRgp",
    "outputId": "bce5209d-90da-4117-c6ac-cda9f3cb3422"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "base_model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,  # Mistral, same as before\n",
    "    quantization_config=bnb_config,  # Same quantization config as before\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    use_auth_token=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_BxOhAiqyRgp"
   },
   "source": [
    "Now load the QLoRA adapter from the appropriate checkpoint directory, i.e. the best performing model checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GwsiqhWuyRgp"
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "ft_model = PeftModel.from_pretrained(base_model, \"mistral-journal-finetune/checkpoint-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lX39ibolyRgp"
   },
   "source": [
    "and run your inference!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UUehsaVNyRgp"
   },
   "source": [
    "Let's try the same `eval_prompt` and thus `model_input` as above, and see if the new finetuned model performs better. I like playing with the repetition penalty (just little tweaks of .01-.05 at a time). THIS IS SO FUN. I'm obsessed wth this AI version of myself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lMkVNEUvyRgp",
    "outputId": "7d49d409-5dbe-4306-c1a4-9d87e3073397"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The following is a note by Eevee the Dog, which doesn't share anything too personal: # \n",
      "Iâ€™m grateful for my best friend coming to visit me. I know weâ€™ll have so much fun and our relationship will continue to flourish. We really are each otherâ€™s number one fan and itâ€™s such a beautiful thing. She supports me in all that I do and celebrates my successes with joy and excitement. I am excited to show her around SF and take her to some of my favorite places. I hope she gets to meet some of my friends here as\n"
     ]
    }
   ],
   "source": [
    "eval_prompt = \" The following is a note by Eevee the Dog, which doesn't share anything too personal: # \"\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "ft_model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=100, repetition_penalty=1.15)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCJnpZoayRgq"
   },
   "source": [
    "### Sweet... it worked! The fine-tuned model now prints out journal entries in my style!\n",
    "\n",
    "How funny to see it write like me as an angsty teenager, and honestly adult. I am obsessed. It knows who my friends are and talks about them, and covers the same topics I usually cover. It's really cool.\n",
    "\n",
    "That output is quite private but I wanted you to see an example run, so I tweaked the `eval_prompt` so that it explicitly wouldn't say anything too sensitive, haha.\n",
    "\n",
    "I hope you enjoyed this tutorial on fine-tuning Mistral on your own data. If you have any questions, feel free to reach out to me on [X](https://x.com/harperscarroll) or [Discord](https://discord.gg/RN2a436M73).\n",
    "\n",
    "ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZjPlpRSJ-aSs"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
