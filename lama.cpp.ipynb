{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2420a84-d124-42a3-a73b-8f6134f4d73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Dec 10 05:33:20 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.29.04              Driver Version: 546.17       CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        On  | 00000000:01:00.0  On |                  Off |\n",
      "|  0%   33C    P8              27W / 450W |   2027MiB / 24564MiB |      3%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af4ce05-00cf-48d0-9ba3-6d0d0c8658f6",
   "metadata": {},
   "source": [
    "# Merge Model and Lora\n",
    "\n",
    "Adapted from https://github.com/brevdev/notebooks/blob/main/gguf-export.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1558c012-66d1-4405-9643-85c4c11b16cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80895324013340198eb1e337e09bf299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./merged_model/tokenizer_config.json',\n",
       " './merged_model/special_tokens_map.json',\n",
       " './merged_model/tokenizer.model',\n",
       " './merged_model/added_tokens.json',\n",
       " './merged_model/tokenizer.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, \"mistral-discern-finetune/checkpoint-11000\")\n",
    " \n",
    "model = model.merge_and_unload()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    " \n",
    "model.save_pretrained(\"./merged_model\")\n",
    "tokenizer.save_pretrained(\"./merged_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfd752e-10a4-4292-87be-fc7a4859ef83",
   "metadata": {},
   "source": [
    "## Convert to GGUF\n",
    "\n",
    "Probably need to restart the kernel here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1caebad4-1477-425f-b0c5-6ead25657c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 http://deb.debian.org/debian bullseye InRelease\n",
      "Hit:2 http://deb.debian.org/debian-security bullseye-security InRelease\n",
      "Hit:3 http://deb.debian.org/debian bullseye-updates InRelease\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "1 package can be upgraded. Run 'apt list --upgradable' to see it.\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "curl is already the newest version (7.74.0-1.3+deb11u10).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 50517  100 50517    0     0   139k      0 --:--:-- --:--:-- --:--:--  139k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100    48  100    48    0     0    296      0 --:--:-- --:--:-- --:--:-- 48000\n",
      "Requirement already satisfied: numpy==1.24.4 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 1)) (1.24.4)\n",
      "Requirement already satisfied: sentencepiece==0.1.98 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (0.1.98)\n",
      "Requirement already satisfied: gguf>=0.1.0 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (0.5.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!apt update\n",
    "!apt install curl -y \n",
    "!curl -L -o convert.py https://github.com/ggerganov/llama.cpp/raw/master/convert.py\n",
    "!curl -L -o requirements.txt https://github.com/ggerganov/llama.cpp/raw/master/requirements.txt\n",
    "\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07166bd4-6ce0-4b5d-9945-c23ece2f5f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -L -o quantize.pl https://github.com/ggerganov/llama.cpp/raw/master/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46eaf93b-a3c1-4c9d-8893-5519f62e9bb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model file merged_model/model-00001-of-00003.safetensors\n",
      "Loading model file merged_model/model-00001-of-00003.safetensors\n",
      "Loading model file merged_model/model-00002-of-00003.safetensors\n",
      "Loading model file merged_model/model-00003-of-00003.safetensors\n",
      "params = Params(n_vocab=32000, n_embd=4096, n_layer=32, n_ctx=32768, n_ff=14336, n_head=32, n_head_kv=8, f_norm_eps=1e-05, rope_scaling_type=None, f_rope_freq_base=10000.0, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=None, path_model=PosixPath('merged_model'))\n",
      "Loading vocab file 'merged_model/tokenizer.model', type 'spm'\n",
      "Permuting layer 0\n",
      "Permuting layer 1\n",
      "Permuting layer 2\n",
      "Permuting layer 3\n",
      "Permuting layer 4\n",
      "Permuting layer 5\n",
      "Permuting layer 6\n",
      "Permuting layer 7\n",
      "Permuting layer 8\n",
      "Permuting layer 9\n",
      "Permuting layer 10\n",
      "Permuting layer 11\n",
      "Permuting layer 12\n",
      "Permuting layer 13\n",
      "Permuting layer 14\n",
      "Permuting layer 15\n",
      "Permuting layer 16\n",
      "Permuting layer 17\n",
      "Permuting layer 18\n",
      "Permuting layer 19\n",
      "Permuting layer 20\n",
      "Permuting layer 21\n",
      "Permuting layer 22\n",
      "Permuting layer 23\n",
      "Permuting layer 24\n",
      "Permuting layer 25\n",
      "Permuting layer 26\n",
      "Permuting layer 27\n",
      "Permuting layer 28\n",
      "Permuting layer 29\n",
      "Permuting layer 30\n",
      "Permuting layer 31\n",
      "model.embed_tokens.weight                        -> token_embd.weight                        | F16    | [32000, 4096]\n",
      "model.layers.0.input_layernorm.weight            -> blk.0.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.0.mlp.down_proj.weight              -> blk.0.ffn_down.weight                    | F16    | [4096, 14336]\n",
      "model.layers.0.mlp.gate_proj.weight              -> blk.0.ffn_gate.weight                    | F16    | [14336, 4096]\n",
      "model.layers.0.mlp.up_proj.weight                -> blk.0.ffn_up.weight                      | F16    | [14336, 4096]\n",
      "model.layers.0.post_attention_layernorm.weight   -> blk.0.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.0.self_attn.k_proj.weight           -> blk.0.attn_k.weight                      | F16    | [1024, 4096]\n",
      "model.layers.0.self_attn.o_proj.weight           -> blk.0.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.0.self_attn.q_proj.weight           -> blk.0.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.0.self_attn.v_proj.weight           -> blk.0.attn_v.weight                      | F16    | [1024, 4096]\n",
      "model.layers.1.input_layernorm.weight            -> blk.1.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.1.mlp.down_proj.weight              -> blk.1.ffn_down.weight                    | F16    | [4096, 14336]\n",
      "model.layers.1.mlp.gate_proj.weight              -> blk.1.ffn_gate.weight                    | F16    | [14336, 4096]\n",
      "model.layers.1.mlp.up_proj.weight                -> blk.1.ffn_up.weight                      | F16    | [14336, 4096]\n",
      "model.layers.1.post_attention_layernorm.weight   -> blk.1.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.1.self_attn.k_proj.weight           -> blk.1.attn_k.weight                      | F16    | [1024, 4096]\n",
      "model.layers.1.self_attn.o_proj.weight           -> blk.1.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.1.self_attn.q_proj.weight           -> blk.1.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.1.self_attn.v_proj.weight           -> blk.1.attn_v.weight                      | F16    | [1024, 4096]\n",
      "model.layers.10.mlp.gate_proj.weight             -> blk.10.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.10.mlp.up_proj.weight               -> blk.10.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.10.self_attn.k_proj.weight          -> blk.10.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.10.self_attn.o_proj.weight          -> blk.10.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.10.self_attn.q_proj.weight          -> blk.10.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.10.self_attn.v_proj.weight          -> blk.10.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.2.input_layernorm.weight            -> blk.2.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.2.mlp.down_proj.weight              -> blk.2.ffn_down.weight                    | F16    | [4096, 14336]\n",
      "model.layers.2.mlp.gate_proj.weight              -> blk.2.ffn_gate.weight                    | F16    | [14336, 4096]\n",
      "model.layers.2.mlp.up_proj.weight                -> blk.2.ffn_up.weight                      | F16    | [14336, 4096]\n",
      "model.layers.2.post_attention_layernorm.weight   -> blk.2.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.2.self_attn.k_proj.weight           -> blk.2.attn_k.weight                      | F16    | [1024, 4096]\n",
      "model.layers.2.self_attn.o_proj.weight           -> blk.2.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.2.self_attn.q_proj.weight           -> blk.2.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.2.self_attn.v_proj.weight           -> blk.2.attn_v.weight                      | F16    | [1024, 4096]\n",
      "model.layers.3.input_layernorm.weight            -> blk.3.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.3.mlp.down_proj.weight              -> blk.3.ffn_down.weight                    | F16    | [4096, 14336]\n",
      "model.layers.3.mlp.gate_proj.weight              -> blk.3.ffn_gate.weight                    | F16    | [14336, 4096]\n",
      "model.layers.3.mlp.up_proj.weight                -> blk.3.ffn_up.weight                      | F16    | [14336, 4096]\n",
      "model.layers.3.post_attention_layernorm.weight   -> blk.3.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.3.self_attn.k_proj.weight           -> blk.3.attn_k.weight                      | F16    | [1024, 4096]\n",
      "model.layers.3.self_attn.o_proj.weight           -> blk.3.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.3.self_attn.q_proj.weight           -> blk.3.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.3.self_attn.v_proj.weight           -> blk.3.attn_v.weight                      | F16    | [1024, 4096]\n",
      "model.layers.4.input_layernorm.weight            -> blk.4.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.4.mlp.down_proj.weight              -> blk.4.ffn_down.weight                    | F16    | [4096, 14336]\n",
      "model.layers.4.mlp.gate_proj.weight              -> blk.4.ffn_gate.weight                    | F16    | [14336, 4096]\n",
      "model.layers.4.mlp.up_proj.weight                -> blk.4.ffn_up.weight                      | F16    | [14336, 4096]\n",
      "model.layers.4.post_attention_layernorm.weight   -> blk.4.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.4.self_attn.k_proj.weight           -> blk.4.attn_k.weight                      | F16    | [1024, 4096]\n",
      "model.layers.4.self_attn.o_proj.weight           -> blk.4.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.4.self_attn.q_proj.weight           -> blk.4.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.4.self_attn.v_proj.weight           -> blk.4.attn_v.weight                      | F16    | [1024, 4096]\n",
      "model.layers.5.input_layernorm.weight            -> blk.5.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.5.mlp.down_proj.weight              -> blk.5.ffn_down.weight                    | F16    | [4096, 14336]\n",
      "model.layers.5.mlp.gate_proj.weight              -> blk.5.ffn_gate.weight                    | F16    | [14336, 4096]\n",
      "model.layers.5.mlp.up_proj.weight                -> blk.5.ffn_up.weight                      | F16    | [14336, 4096]\n",
      "model.layers.5.post_attention_layernorm.weight   -> blk.5.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.5.self_attn.k_proj.weight           -> blk.5.attn_k.weight                      | F16    | [1024, 4096]\n",
      "model.layers.5.self_attn.o_proj.weight           -> blk.5.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.5.self_attn.q_proj.weight           -> blk.5.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.5.self_attn.v_proj.weight           -> blk.5.attn_v.weight                      | F16    | [1024, 4096]\n",
      "model.layers.6.input_layernorm.weight            -> blk.6.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.6.mlp.down_proj.weight              -> blk.6.ffn_down.weight                    | F16    | [4096, 14336]\n",
      "model.layers.6.mlp.gate_proj.weight              -> blk.6.ffn_gate.weight                    | F16    | [14336, 4096]\n",
      "model.layers.6.mlp.up_proj.weight                -> blk.6.ffn_up.weight                      | F16    | [14336, 4096]\n",
      "model.layers.6.post_attention_layernorm.weight   -> blk.6.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.6.self_attn.k_proj.weight           -> blk.6.attn_k.weight                      | F16    | [1024, 4096]\n",
      "model.layers.6.self_attn.o_proj.weight           -> blk.6.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.6.self_attn.q_proj.weight           -> blk.6.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.6.self_attn.v_proj.weight           -> blk.6.attn_v.weight                      | F16    | [1024, 4096]\n",
      "model.layers.7.input_layernorm.weight            -> blk.7.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.7.mlp.down_proj.weight              -> blk.7.ffn_down.weight                    | F16    | [4096, 14336]\n",
      "model.layers.7.mlp.gate_proj.weight              -> blk.7.ffn_gate.weight                    | F16    | [14336, 4096]\n",
      "model.layers.7.mlp.up_proj.weight                -> blk.7.ffn_up.weight                      | F16    | [14336, 4096]\n",
      "model.layers.7.post_attention_layernorm.weight   -> blk.7.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.7.self_attn.k_proj.weight           -> blk.7.attn_k.weight                      | F16    | [1024, 4096]\n",
      "model.layers.7.self_attn.o_proj.weight           -> blk.7.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.7.self_attn.q_proj.weight           -> blk.7.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.7.self_attn.v_proj.weight           -> blk.7.attn_v.weight                      | F16    | [1024, 4096]\n",
      "model.layers.8.input_layernorm.weight            -> blk.8.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.8.mlp.down_proj.weight              -> blk.8.ffn_down.weight                    | F16    | [4096, 14336]\n",
      "model.layers.8.mlp.gate_proj.weight              -> blk.8.ffn_gate.weight                    | F16    | [14336, 4096]\n",
      "model.layers.8.mlp.up_proj.weight                -> blk.8.ffn_up.weight                      | F16    | [14336, 4096]\n",
      "model.layers.8.post_attention_layernorm.weight   -> blk.8.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.8.self_attn.k_proj.weight           -> blk.8.attn_k.weight                      | F16    | [1024, 4096]\n",
      "model.layers.8.self_attn.o_proj.weight           -> blk.8.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.8.self_attn.q_proj.weight           -> blk.8.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.8.self_attn.v_proj.weight           -> blk.8.attn_v.weight                      | F16    | [1024, 4096]\n",
      "model.layers.9.input_layernorm.weight            -> blk.9.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.9.mlp.down_proj.weight              -> blk.9.ffn_down.weight                    | F16    | [4096, 14336]\n",
      "model.layers.9.mlp.gate_proj.weight              -> blk.9.ffn_gate.weight                    | F16    | [14336, 4096]\n",
      "model.layers.9.mlp.up_proj.weight                -> blk.9.ffn_up.weight                      | F16    | [14336, 4096]\n",
      "model.layers.9.post_attention_layernorm.weight   -> blk.9.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.9.self_attn.k_proj.weight           -> blk.9.attn_k.weight                      | F16    | [1024, 4096]\n",
      "model.layers.9.self_attn.o_proj.weight           -> blk.9.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.9.self_attn.q_proj.weight           -> blk.9.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.9.self_attn.v_proj.weight           -> blk.9.attn_v.weight                      | F16    | [1024, 4096]\n",
      "model.layers.10.input_layernorm.weight           -> blk.10.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.10.mlp.down_proj.weight             -> blk.10.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.10.post_attention_layernorm.weight  -> blk.10.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.11.input_layernorm.weight           -> blk.11.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.11.mlp.down_proj.weight             -> blk.11.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.11.mlp.gate_proj.weight             -> blk.11.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.11.mlp.up_proj.weight               -> blk.11.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.11.post_attention_layernorm.weight  -> blk.11.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.11.self_attn.k_proj.weight          -> blk.11.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.11.self_attn.o_proj.weight          -> blk.11.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.11.self_attn.q_proj.weight          -> blk.11.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.11.self_attn.v_proj.weight          -> blk.11.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.12.input_layernorm.weight           -> blk.12.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.12.mlp.down_proj.weight             -> blk.12.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.12.mlp.gate_proj.weight             -> blk.12.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.12.mlp.up_proj.weight               -> blk.12.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.12.post_attention_layernorm.weight  -> blk.12.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.12.self_attn.k_proj.weight          -> blk.12.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.12.self_attn.o_proj.weight          -> blk.12.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.12.self_attn.q_proj.weight          -> blk.12.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.12.self_attn.v_proj.weight          -> blk.12.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.13.input_layernorm.weight           -> blk.13.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.13.mlp.down_proj.weight             -> blk.13.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.13.mlp.gate_proj.weight             -> blk.13.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.13.mlp.up_proj.weight               -> blk.13.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.13.post_attention_layernorm.weight  -> blk.13.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.13.self_attn.k_proj.weight          -> blk.13.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.13.self_attn.o_proj.weight          -> blk.13.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.13.self_attn.q_proj.weight          -> blk.13.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.13.self_attn.v_proj.weight          -> blk.13.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.14.input_layernorm.weight           -> blk.14.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.14.mlp.down_proj.weight             -> blk.14.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.14.mlp.gate_proj.weight             -> blk.14.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.14.mlp.up_proj.weight               -> blk.14.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.14.post_attention_layernorm.weight  -> blk.14.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.14.self_attn.k_proj.weight          -> blk.14.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.14.self_attn.o_proj.weight          -> blk.14.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.14.self_attn.q_proj.weight          -> blk.14.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.14.self_attn.v_proj.weight          -> blk.14.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.15.input_layernorm.weight           -> blk.15.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.15.mlp.down_proj.weight             -> blk.15.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.15.mlp.gate_proj.weight             -> blk.15.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.15.mlp.up_proj.weight               -> blk.15.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.15.post_attention_layernorm.weight  -> blk.15.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.15.self_attn.k_proj.weight          -> blk.15.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.15.self_attn.o_proj.weight          -> blk.15.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.15.self_attn.q_proj.weight          -> blk.15.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.15.self_attn.v_proj.weight          -> blk.15.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.16.input_layernorm.weight           -> blk.16.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.16.mlp.down_proj.weight             -> blk.16.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.16.mlp.gate_proj.weight             -> blk.16.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.16.mlp.up_proj.weight               -> blk.16.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.16.post_attention_layernorm.weight  -> blk.16.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.16.self_attn.k_proj.weight          -> blk.16.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.16.self_attn.o_proj.weight          -> blk.16.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.16.self_attn.q_proj.weight          -> blk.16.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.16.self_attn.v_proj.weight          -> blk.16.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.17.input_layernorm.weight           -> blk.17.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.17.mlp.down_proj.weight             -> blk.17.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.17.mlp.gate_proj.weight             -> blk.17.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.17.mlp.up_proj.weight               -> blk.17.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.17.post_attention_layernorm.weight  -> blk.17.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.17.self_attn.k_proj.weight          -> blk.17.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.17.self_attn.o_proj.weight          -> blk.17.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.17.self_attn.q_proj.weight          -> blk.17.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.17.self_attn.v_proj.weight          -> blk.17.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.18.input_layernorm.weight           -> blk.18.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.18.mlp.down_proj.weight             -> blk.18.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.18.mlp.gate_proj.weight             -> blk.18.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.18.mlp.up_proj.weight               -> blk.18.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.18.post_attention_layernorm.weight  -> blk.18.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.18.self_attn.k_proj.weight          -> blk.18.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.18.self_attn.o_proj.weight          -> blk.18.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.18.self_attn.q_proj.weight          -> blk.18.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.18.self_attn.v_proj.weight          -> blk.18.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.19.input_layernorm.weight           -> blk.19.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.19.mlp.down_proj.weight             -> blk.19.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.19.mlp.gate_proj.weight             -> blk.19.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.19.mlp.up_proj.weight               -> blk.19.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.19.post_attention_layernorm.weight  -> blk.19.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.19.self_attn.k_proj.weight          -> blk.19.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.19.self_attn.o_proj.weight          -> blk.19.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.19.self_attn.q_proj.weight          -> blk.19.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.19.self_attn.v_proj.weight          -> blk.19.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.20.input_layernorm.weight           -> blk.20.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.20.mlp.down_proj.weight             -> blk.20.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.20.mlp.gate_proj.weight             -> blk.20.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.20.mlp.up_proj.weight               -> blk.20.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.20.post_attention_layernorm.weight  -> blk.20.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.20.self_attn.k_proj.weight          -> blk.20.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.20.self_attn.o_proj.weight          -> blk.20.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.20.self_attn.q_proj.weight          -> blk.20.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.20.self_attn.v_proj.weight          -> blk.20.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.21.input_layernorm.weight           -> blk.21.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.21.mlp.down_proj.weight             -> blk.21.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.21.mlp.gate_proj.weight             -> blk.21.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.21.mlp.up_proj.weight               -> blk.21.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.21.post_attention_layernorm.weight  -> blk.21.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.21.self_attn.k_proj.weight          -> blk.21.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.21.self_attn.o_proj.weight          -> blk.21.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.21.self_attn.q_proj.weight          -> blk.21.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.21.self_attn.v_proj.weight          -> blk.21.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.22.self_attn.k_proj.weight          -> blk.22.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.22.self_attn.o_proj.weight          -> blk.22.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.22.self_attn.q_proj.weight          -> blk.22.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.22.self_attn.v_proj.weight          -> blk.22.attn_v.weight                     | F16    | [1024, 4096]\n",
      "lm_head.weight                                   -> output.weight                            | F16    | [32000, 4096]\n",
      "model.layers.22.input_layernorm.weight           -> blk.22.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.22.mlp.down_proj.weight             -> blk.22.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.22.mlp.gate_proj.weight             -> blk.22.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.22.mlp.up_proj.weight               -> blk.22.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.22.post_attention_layernorm.weight  -> blk.22.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.23.input_layernorm.weight           -> blk.23.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.23.mlp.down_proj.weight             -> blk.23.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.23.mlp.gate_proj.weight             -> blk.23.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.23.mlp.up_proj.weight               -> blk.23.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.23.post_attention_layernorm.weight  -> blk.23.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.23.self_attn.k_proj.weight          -> blk.23.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.23.self_attn.o_proj.weight          -> blk.23.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.23.self_attn.q_proj.weight          -> blk.23.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.23.self_attn.v_proj.weight          -> blk.23.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.24.input_layernorm.weight           -> blk.24.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.24.mlp.down_proj.weight             -> blk.24.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.24.mlp.gate_proj.weight             -> blk.24.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.24.mlp.up_proj.weight               -> blk.24.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.24.post_attention_layernorm.weight  -> blk.24.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.24.self_attn.k_proj.weight          -> blk.24.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.24.self_attn.o_proj.weight          -> blk.24.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.24.self_attn.q_proj.weight          -> blk.24.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.24.self_attn.v_proj.weight          -> blk.24.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.25.input_layernorm.weight           -> blk.25.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.25.mlp.down_proj.weight             -> blk.25.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.25.mlp.gate_proj.weight             -> blk.25.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.25.mlp.up_proj.weight               -> blk.25.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.25.post_attention_layernorm.weight  -> blk.25.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.25.self_attn.k_proj.weight          -> blk.25.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.25.self_attn.o_proj.weight          -> blk.25.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.25.self_attn.q_proj.weight          -> blk.25.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.25.self_attn.v_proj.weight          -> blk.25.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.26.input_layernorm.weight           -> blk.26.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.26.mlp.down_proj.weight             -> blk.26.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.26.mlp.gate_proj.weight             -> blk.26.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.26.mlp.up_proj.weight               -> blk.26.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.26.post_attention_layernorm.weight  -> blk.26.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.26.self_attn.k_proj.weight          -> blk.26.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.26.self_attn.o_proj.weight          -> blk.26.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.26.self_attn.q_proj.weight          -> blk.26.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.26.self_attn.v_proj.weight          -> blk.26.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.27.input_layernorm.weight           -> blk.27.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.27.mlp.down_proj.weight             -> blk.27.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.27.mlp.gate_proj.weight             -> blk.27.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.27.mlp.up_proj.weight               -> blk.27.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.27.post_attention_layernorm.weight  -> blk.27.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.27.self_attn.k_proj.weight          -> blk.27.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.27.self_attn.o_proj.weight          -> blk.27.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.27.self_attn.q_proj.weight          -> blk.27.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.27.self_attn.v_proj.weight          -> blk.27.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.28.input_layernorm.weight           -> blk.28.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.28.mlp.down_proj.weight             -> blk.28.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.28.mlp.gate_proj.weight             -> blk.28.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.28.mlp.up_proj.weight               -> blk.28.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.28.post_attention_layernorm.weight  -> blk.28.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.28.self_attn.k_proj.weight          -> blk.28.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.28.self_attn.o_proj.weight          -> blk.28.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.28.self_attn.q_proj.weight          -> blk.28.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.28.self_attn.v_proj.weight          -> blk.28.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.29.input_layernorm.weight           -> blk.29.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.29.mlp.down_proj.weight             -> blk.29.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.29.mlp.gate_proj.weight             -> blk.29.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.29.mlp.up_proj.weight               -> blk.29.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.29.post_attention_layernorm.weight  -> blk.29.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.29.self_attn.k_proj.weight          -> blk.29.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.29.self_attn.o_proj.weight          -> blk.29.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.29.self_attn.q_proj.weight          -> blk.29.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.29.self_attn.v_proj.weight          -> blk.29.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.30.input_layernorm.weight           -> blk.30.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.30.mlp.down_proj.weight             -> blk.30.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.30.mlp.gate_proj.weight             -> blk.30.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.30.mlp.up_proj.weight               -> blk.30.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.30.post_attention_layernorm.weight  -> blk.30.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.30.self_attn.k_proj.weight          -> blk.30.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.30.self_attn.o_proj.weight          -> blk.30.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.30.self_attn.q_proj.weight          -> blk.30.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.30.self_attn.v_proj.weight          -> blk.30.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.31.input_layernorm.weight           -> blk.31.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.31.mlp.down_proj.weight             -> blk.31.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.31.mlp.gate_proj.weight             -> blk.31.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.31.mlp.up_proj.weight               -> blk.31.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.31.post_attention_layernorm.weight  -> blk.31.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.31.self_attn.k_proj.weight          -> blk.31.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.31.self_attn.o_proj.weight          -> blk.31.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.31.self_attn.q_proj.weight          -> blk.31.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.31.self_attn.v_proj.weight          -> blk.31.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.norm.weight                                -> output_norm.weight                       | F16    | [4096]\n",
      "Writing merged_model/ggml-model-f16.gguf, format 1\n",
      "gguf: This GGUF file is for Little Endian only\n",
      "gguf: Setting special token type bos to 1\n",
      "gguf: Setting special token type eos to 2\n",
      "gguf: Setting special token type unk to 0\n",
      "gguf: Setting add_bos_token to True\n",
      "gguf: Setting add_eos_token to False\n",
      "[  1/291] Writing tensor token_embd.weight                      | size  32000 x   4096  | type F16  | T+   5\n",
      "[  2/291] Writing tensor blk.0.attn_norm.weight                 | size   4096           | type F32  | T+   5\n",
      "[  3/291] Writing tensor blk.0.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+   5\n",
      "[  4/291] Writing tensor blk.0.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+   5\n",
      "[  5/291] Writing tensor blk.0.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+   5\n",
      "[  6/291] Writing tensor blk.0.ffn_norm.weight                  | size   4096           | type F32  | T+   5\n",
      "[  7/291] Writing tensor blk.0.attn_k.weight                    | size   1024 x   4096  | type F16  | T+   5\n",
      "[  8/291] Writing tensor blk.0.attn_output.weight               | size   4096 x   4096  | type F16  | T+   5\n",
      "[  9/291] Writing tensor blk.0.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   6\n",
      "[ 10/291] Writing tensor blk.0.attn_v.weight                    | size   1024 x   4096  | type F16  | T+   6\n",
      "[ 11/291] Writing tensor blk.1.attn_norm.weight                 | size   4096           | type F32  | T+   6\n",
      "[ 12/291] Writing tensor blk.1.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+   8\n",
      "[ 13/291] Writing tensor blk.1.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+   8\n",
      "[ 14/291] Writing tensor blk.1.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+   8\n",
      "[ 15/291] Writing tensor blk.1.ffn_norm.weight                  | size   4096           | type F32  | T+   8\n",
      "[ 16/291] Writing tensor blk.1.attn_k.weight                    | size   1024 x   4096  | type F16  | T+   8\n",
      "[ 17/291] Writing tensor blk.1.attn_output.weight               | size   4096 x   4096  | type F16  | T+   8\n",
      "[ 18/291] Writing tensor blk.1.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   8\n",
      "[ 19/291] Writing tensor blk.1.attn_v.weight                    | size   1024 x   4096  | type F16  | T+   8\n",
      "[ 20/291] Writing tensor blk.10.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  11\n",
      "[ 21/291] Writing tensor blk.10.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  11\n",
      "[ 22/291] Writing tensor blk.10.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  11\n",
      "[ 23/291] Writing tensor blk.10.attn_output.weight              | size   4096 x   4096  | type F16  | T+  11\n",
      "[ 24/291] Writing tensor blk.10.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  11\n",
      "[ 25/291] Writing tensor blk.10.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  11\n",
      "[ 26/291] Writing tensor blk.2.attn_norm.weight                 | size   4096           | type F32  | T+  11\n",
      "[ 27/291] Writing tensor blk.2.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+  11\n",
      "[ 28/291] Writing tensor blk.2.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+  13\n",
      "[ 29/291] Writing tensor blk.2.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+  13\n",
      "[ 30/291] Writing tensor blk.2.ffn_norm.weight                  | size   4096           | type F32  | T+  13\n",
      "[ 31/291] Writing tensor blk.2.attn_k.weight                    | size   1024 x   4096  | type F16  | T+  13\n",
      "[ 32/291] Writing tensor blk.2.attn_output.weight               | size   4096 x   4096  | type F16  | T+  13\n",
      "[ 33/291] Writing tensor blk.2.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  13\n",
      "[ 34/291] Writing tensor blk.2.attn_v.weight                    | size   1024 x   4096  | type F16  | T+  13\n",
      "[ 35/291] Writing tensor blk.3.attn_norm.weight                 | size   4096           | type F32  | T+  13\n",
      "[ 36/291] Writing tensor blk.3.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+  15\n",
      "[ 37/291] Writing tensor blk.3.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+  15\n",
      "[ 38/291] Writing tensor blk.3.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+  15\n",
      "[ 39/291] Writing tensor blk.3.ffn_norm.weight                  | size   4096           | type F32  | T+  16\n",
      "[ 40/291] Writing tensor blk.3.attn_k.weight                    | size   1024 x   4096  | type F16  | T+  16\n",
      "[ 41/291] Writing tensor blk.3.attn_output.weight               | size   4096 x   4096  | type F16  | T+  16\n",
      "[ 42/291] Writing tensor blk.3.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  16\n",
      "[ 43/291] Writing tensor blk.3.attn_v.weight                    | size   1024 x   4096  | type F16  | T+  16\n",
      "[ 44/291] Writing tensor blk.4.attn_norm.weight                 | size   4096           | type F32  | T+  16\n",
      "[ 45/291] Writing tensor blk.4.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+  18\n",
      "[ 46/291] Writing tensor blk.4.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+  18\n",
      "[ 47/291] Writing tensor blk.4.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+  18\n",
      "[ 48/291] Writing tensor blk.4.ffn_norm.weight                  | size   4096           | type F32  | T+  18\n",
      "[ 49/291] Writing tensor blk.4.attn_k.weight                    | size   1024 x   4096  | type F16  | T+  18\n",
      "[ 50/291] Writing tensor blk.4.attn_output.weight               | size   4096 x   4096  | type F16  | T+  18\n",
      "[ 51/291] Writing tensor blk.4.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  18\n",
      "[ 52/291] Writing tensor blk.4.attn_v.weight                    | size   1024 x   4096  | type F16  | T+  18\n",
      "[ 53/291] Writing tensor blk.5.attn_norm.weight                 | size   4096           | type F32  | T+  18\n",
      "[ 54/291] Writing tensor blk.5.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+  20\n",
      "[ 55/291] Writing tensor blk.5.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+  20\n",
      "[ 56/291] Writing tensor blk.5.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+  20\n",
      "[ 57/291] Writing tensor blk.5.ffn_norm.weight                  | size   4096           | type F32  | T+  20\n",
      "[ 58/291] Writing tensor blk.5.attn_k.weight                    | size   1024 x   4096  | type F16  | T+  20\n",
      "[ 59/291] Writing tensor blk.5.attn_output.weight               | size   4096 x   4096  | type F16  | T+  20\n",
      "[ 60/291] Writing tensor blk.5.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  20\n",
      "[ 61/291] Writing tensor blk.5.attn_v.weight                    | size   1024 x   4096  | type F16  | T+  21\n",
      "[ 62/291] Writing tensor blk.6.attn_norm.weight                 | size   4096           | type F32  | T+  21\n",
      "[ 63/291] Writing tensor blk.6.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+  23\n",
      "[ 64/291] Writing tensor blk.6.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+  23\n",
      "[ 65/291] Writing tensor blk.6.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+  23\n",
      "[ 66/291] Writing tensor blk.6.ffn_norm.weight                  | size   4096           | type F32  | T+  23\n",
      "[ 67/291] Writing tensor blk.6.attn_k.weight                    | size   1024 x   4096  | type F16  | T+  23\n",
      "[ 68/291] Writing tensor blk.6.attn_output.weight               | size   4096 x   4096  | type F16  | T+  23\n",
      "[ 69/291] Writing tensor blk.6.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  23\n",
      "[ 70/291] Writing tensor blk.6.attn_v.weight                    | size   1024 x   4096  | type F16  | T+  23\n",
      "[ 71/291] Writing tensor blk.7.attn_norm.weight                 | size   4096           | type F32  | T+  23\n",
      "[ 72/291] Writing tensor blk.7.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+  25\n",
      "[ 73/291] Writing tensor blk.7.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+  25\n",
      "[ 74/291] Writing tensor blk.7.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+  25\n",
      "[ 75/291] Writing tensor blk.7.ffn_norm.weight                  | size   4096           | type F32  | T+  25\n",
      "[ 76/291] Writing tensor blk.7.attn_k.weight                    | size   1024 x   4096  | type F16  | T+  25\n",
      "[ 77/291] Writing tensor blk.7.attn_output.weight               | size   4096 x   4096  | type F16  | T+  25\n",
      "[ 78/291] Writing tensor blk.7.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  25\n",
      "[ 79/291] Writing tensor blk.7.attn_v.weight                    | size   1024 x   4096  | type F16  | T+  25\n",
      "[ 80/291] Writing tensor blk.8.attn_norm.weight                 | size   4096           | type F32  | T+  25\n",
      "[ 81/291] Writing tensor blk.8.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+  28\n",
      "[ 82/291] Writing tensor blk.8.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+  28\n",
      "[ 83/291] Writing tensor blk.8.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+  28\n",
      "[ 84/291] Writing tensor blk.8.ffn_norm.weight                  | size   4096           | type F32  | T+  28\n",
      "[ 85/291] Writing tensor blk.8.attn_k.weight                    | size   1024 x   4096  | type F16  | T+  28\n",
      "[ 86/291] Writing tensor blk.8.attn_output.weight               | size   4096 x   4096  | type F16  | T+  28\n",
      "[ 87/291] Writing tensor blk.8.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  28\n",
      "[ 88/291] Writing tensor blk.8.attn_v.weight                    | size   1024 x   4096  | type F16  | T+  28\n",
      "[ 89/291] Writing tensor blk.9.attn_norm.weight                 | size   4096           | type F32  | T+  28\n",
      "[ 90/291] Writing tensor blk.9.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+  30\n",
      "[ 91/291] Writing tensor blk.9.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+  30\n",
      "[ 92/291] Writing tensor blk.9.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+  30\n",
      "[ 93/291] Writing tensor blk.9.ffn_norm.weight                  | size   4096           | type F32  | T+  30\n",
      "[ 94/291] Writing tensor blk.9.attn_k.weight                    | size   1024 x   4096  | type F16  | T+  30\n",
      "[ 95/291] Writing tensor blk.9.attn_output.weight               | size   4096 x   4096  | type F16  | T+  30\n",
      "[ 96/291] Writing tensor blk.9.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  30\n",
      "[ 97/291] Writing tensor blk.9.attn_v.weight                    | size   1024 x   4096  | type F16  | T+  30\n",
      "[ 98/291] Writing tensor blk.10.attn_norm.weight                | size   4096           | type F32  | T+  30\n",
      "[ 99/291] Writing tensor blk.10.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  33\n",
      "[100/291] Writing tensor blk.10.ffn_norm.weight                 | size   4096           | type F32  | T+  33\n",
      "[101/291] Writing tensor blk.11.attn_norm.weight                | size   4096           | type F32  | T+  33\n",
      "[102/291] Writing tensor blk.11.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  33\n",
      "[103/291] Writing tensor blk.11.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  33\n",
      "[104/291] Writing tensor blk.11.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  33\n",
      "[105/291] Writing tensor blk.11.ffn_norm.weight                 | size   4096           | type F32  | T+  33\n",
      "[106/291] Writing tensor blk.11.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  33\n",
      "[107/291] Writing tensor blk.11.attn_output.weight              | size   4096 x   4096  | type F16  | T+  34\n",
      "[108/291] Writing tensor blk.11.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  34\n",
      "[109/291] Writing tensor blk.11.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  34\n",
      "[110/291] Writing tensor blk.12.attn_norm.weight                | size   4096           | type F32  | T+  34\n",
      "[111/291] Writing tensor blk.12.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  36\n",
      "[112/291] Writing tensor blk.12.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  36\n",
      "[113/291] Writing tensor blk.12.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  36\n",
      "[114/291] Writing tensor blk.12.ffn_norm.weight                 | size   4096           | type F32  | T+  36\n",
      "[115/291] Writing tensor blk.12.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  36\n",
      "[116/291] Writing tensor blk.12.attn_output.weight              | size   4096 x   4096  | type F16  | T+  36\n",
      "[117/291] Writing tensor blk.12.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  36\n",
      "[118/291] Writing tensor blk.12.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  36\n",
      "[119/291] Writing tensor blk.13.attn_norm.weight                | size   4096           | type F32  | T+  36\n",
      "[120/291] Writing tensor blk.13.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  38\n",
      "[121/291] Writing tensor blk.13.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  38\n",
      "[122/291] Writing tensor blk.13.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  38\n",
      "[123/291] Writing tensor blk.13.ffn_norm.weight                 | size   4096           | type F32  | T+  38\n",
      "[124/291] Writing tensor blk.13.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  38\n",
      "[125/291] Writing tensor blk.13.attn_output.weight              | size   4096 x   4096  | type F16  | T+  38\n",
      "[126/291] Writing tensor blk.13.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  38\n",
      "[127/291] Writing tensor blk.13.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  38\n",
      "[128/291] Writing tensor blk.14.attn_norm.weight                | size   4096           | type F32  | T+  38\n",
      "[129/291] Writing tensor blk.14.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  40\n",
      "[130/291] Writing tensor blk.14.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  41\n",
      "[131/291] Writing tensor blk.14.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  41\n",
      "[132/291] Writing tensor blk.14.ffn_norm.weight                 | size   4096           | type F32  | T+  41\n",
      "[133/291] Writing tensor blk.14.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  41\n",
      "[134/291] Writing tensor blk.14.attn_output.weight              | size   4096 x   4096  | type F16  | T+  41\n",
      "[135/291] Writing tensor blk.14.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  41\n",
      "[136/291] Writing tensor blk.14.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  41\n",
      "[137/291] Writing tensor blk.15.attn_norm.weight                | size   4096           | type F32  | T+  41\n",
      "[138/291] Writing tensor blk.15.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  44\n",
      "[139/291] Writing tensor blk.15.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  44\n",
      "[140/291] Writing tensor blk.15.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  44\n",
      "[141/291] Writing tensor blk.15.ffn_norm.weight                 | size   4096           | type F32  | T+  44\n",
      "[142/291] Writing tensor blk.15.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  44\n",
      "[143/291] Writing tensor blk.15.attn_output.weight              | size   4096 x   4096  | type F16  | T+  44\n",
      "[144/291] Writing tensor blk.15.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  44\n",
      "[145/291] Writing tensor blk.15.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  44\n",
      "[146/291] Writing tensor blk.16.attn_norm.weight                | size   4096           | type F32  | T+  44\n",
      "[147/291] Writing tensor blk.16.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  47\n",
      "[148/291] Writing tensor blk.16.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  48\n",
      "[149/291] Writing tensor blk.16.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  48\n",
      "[150/291] Writing tensor blk.16.ffn_norm.weight                 | size   4096           | type F32  | T+  48\n",
      "[151/291] Writing tensor blk.16.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  48\n",
      "[152/291] Writing tensor blk.16.attn_output.weight              | size   4096 x   4096  | type F16  | T+  48\n",
      "[153/291] Writing tensor blk.16.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  48\n",
      "[154/291] Writing tensor blk.16.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  48\n",
      "[155/291] Writing tensor blk.17.attn_norm.weight                | size   4096           | type F32  | T+  48\n",
      "[156/291] Writing tensor blk.17.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  51\n",
      "[157/291] Writing tensor blk.17.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  51\n",
      "[158/291] Writing tensor blk.17.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  51\n",
      "[159/291] Writing tensor blk.17.ffn_norm.weight                 | size   4096           | type F32  | T+  51\n",
      "[160/291] Writing tensor blk.17.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  51\n",
      "[161/291] Writing tensor blk.17.attn_output.weight              | size   4096 x   4096  | type F16  | T+  51\n",
      "[162/291] Writing tensor blk.17.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  51\n",
      "[163/291] Writing tensor blk.17.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  51\n",
      "[164/291] Writing tensor blk.18.attn_norm.weight                | size   4096           | type F32  | T+  51\n",
      "[165/291] Writing tensor blk.18.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  55\n",
      "[166/291] Writing tensor blk.18.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  55\n",
      "[167/291] Writing tensor blk.18.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  55\n",
      "[168/291] Writing tensor blk.18.ffn_norm.weight                 | size   4096           | type F32  | T+  55\n",
      "[169/291] Writing tensor blk.18.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  55\n",
      "[170/291] Writing tensor blk.18.attn_output.weight              | size   4096 x   4096  | type F16  | T+  55\n",
      "[171/291] Writing tensor blk.18.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  55\n",
      "[172/291] Writing tensor blk.18.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  55\n",
      "[173/291] Writing tensor blk.19.attn_norm.weight                | size   4096           | type F32  | T+  55\n",
      "[174/291] Writing tensor blk.19.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  59\n",
      "[175/291] Writing tensor blk.19.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  59\n",
      "[176/291] Writing tensor blk.19.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  59\n",
      "[177/291] Writing tensor blk.19.ffn_norm.weight                 | size   4096           | type F32  | T+  59\n",
      "[178/291] Writing tensor blk.19.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  59\n",
      "[179/291] Writing tensor blk.19.attn_output.weight              | size   4096 x   4096  | type F16  | T+  59\n",
      "[180/291] Writing tensor blk.19.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  59\n",
      "[181/291] Writing tensor blk.19.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  59\n",
      "[182/291] Writing tensor blk.20.attn_norm.weight                | size   4096           | type F32  | T+  59\n",
      "[183/291] Writing tensor blk.20.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  62\n",
      "[184/291] Writing tensor blk.20.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  63\n",
      "[185/291] Writing tensor blk.20.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  63\n",
      "[186/291] Writing tensor blk.20.ffn_norm.weight                 | size   4096           | type F32  | T+  63\n",
      "[187/291] Writing tensor blk.20.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  63\n",
      "[188/291] Writing tensor blk.20.attn_output.weight              | size   4096 x   4096  | type F16  | T+  63\n",
      "[189/291] Writing tensor blk.20.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  63\n",
      "[190/291] Writing tensor blk.20.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  63\n",
      "[191/291] Writing tensor blk.21.attn_norm.weight                | size   4096           | type F32  | T+  63\n",
      "[192/291] Writing tensor blk.21.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  66\n",
      "[193/291] Writing tensor blk.21.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  66\n",
      "[194/291] Writing tensor blk.21.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  66\n",
      "[195/291] Writing tensor blk.21.ffn_norm.weight                 | size   4096           | type F32  | T+  67\n",
      "[196/291] Writing tensor blk.21.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  67\n",
      "[197/291] Writing tensor blk.21.attn_output.weight              | size   4096 x   4096  | type F16  | T+  67\n",
      "[198/291] Writing tensor blk.21.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  67\n",
      "[199/291] Writing tensor blk.21.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  67\n",
      "[200/291] Writing tensor blk.22.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  67\n",
      "[201/291] Writing tensor blk.22.attn_output.weight              | size   4096 x   4096  | type F16  | T+  67\n",
      "[202/291] Writing tensor blk.22.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  68\n",
      "[203/291] Writing tensor blk.22.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  68\n",
      "[204/291] Writing tensor output.weight                          | size  32000 x   4096  | type F16  | T+  74\n",
      "[205/291] Writing tensor blk.22.attn_norm.weight                | size   4096           | type F32  | T+  74\n",
      "[206/291] Writing tensor blk.22.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  74\n",
      "[207/291] Writing tensor blk.22.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  74\n",
      "[208/291] Writing tensor blk.22.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  74\n",
      "[209/291] Writing tensor blk.22.ffn_norm.weight                 | size   4096           | type F32  | T+  74\n",
      "[210/291] Writing tensor blk.23.attn_norm.weight                | size   4096           | type F32  | T+  74\n",
      "[211/291] Writing tensor blk.23.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  74\n",
      "[212/291] Writing tensor blk.23.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  77\n",
      "[213/291] Writing tensor blk.23.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  78\n",
      "[214/291] Writing tensor blk.23.ffn_norm.weight                 | size   4096           | type F32  | T+  78\n",
      "[215/291] Writing tensor blk.23.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  78\n",
      "[216/291] Writing tensor blk.23.attn_output.weight              | size   4096 x   4096  | type F16  | T+  78\n",
      "[217/291] Writing tensor blk.23.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  78\n",
      "[218/291] Writing tensor blk.23.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  78\n",
      "[219/291] Writing tensor blk.24.attn_norm.weight                | size   4096           | type F32  | T+  78\n",
      "[220/291] Writing tensor blk.24.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  81\n",
      "[221/291] Writing tensor blk.24.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  81\n",
      "[222/291] Writing tensor blk.24.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  81\n",
      "[223/291] Writing tensor blk.24.ffn_norm.weight                 | size   4096           | type F32  | T+  81\n",
      "[224/291] Writing tensor blk.24.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  81\n",
      "[225/291] Writing tensor blk.24.attn_output.weight              | size   4096 x   4096  | type F16  | T+  81\n",
      "[226/291] Writing tensor blk.24.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  81\n",
      "[227/291] Writing tensor blk.24.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  81\n",
      "[228/291] Writing tensor blk.25.attn_norm.weight                | size   4096           | type F32  | T+  81\n",
      "[229/291] Writing tensor blk.25.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  85\n",
      "[230/291] Writing tensor blk.25.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  85\n",
      "[231/291] Writing tensor blk.25.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  85\n",
      "[232/291] Writing tensor blk.25.ffn_norm.weight                 | size   4096           | type F32  | T+  85\n",
      "[233/291] Writing tensor blk.25.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  85\n",
      "[234/291] Writing tensor blk.25.attn_output.weight              | size   4096 x   4096  | type F16  | T+  85\n",
      "[235/291] Writing tensor blk.25.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  85\n",
      "[236/291] Writing tensor blk.25.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  85\n",
      "[237/291] Writing tensor blk.26.attn_norm.weight                | size   4096           | type F32  | T+  85\n",
      "[238/291] Writing tensor blk.26.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  89\n",
      "[239/291] Writing tensor blk.26.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  89\n",
      "[240/291] Writing tensor blk.26.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  89\n",
      "[241/291] Writing tensor blk.26.ffn_norm.weight                 | size   4096           | type F32  | T+  89\n",
      "[242/291] Writing tensor blk.26.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  89\n",
      "[243/291] Writing tensor blk.26.attn_output.weight              | size   4096 x   4096  | type F16  | T+  89\n",
      "[244/291] Writing tensor blk.26.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  89\n",
      "[245/291] Writing tensor blk.26.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  89\n",
      "[246/291] Writing tensor blk.27.attn_norm.weight                | size   4096           | type F32  | T+  89\n",
      "[247/291] Writing tensor blk.27.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  93\n",
      "[248/291] Writing tensor blk.27.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  93\n",
      "[249/291] Writing tensor blk.27.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  93\n",
      "[250/291] Writing tensor blk.27.ffn_norm.weight                 | size   4096           | type F32  | T+  93\n",
      "[251/291] Writing tensor blk.27.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  93\n",
      "[252/291] Writing tensor blk.27.attn_output.weight              | size   4096 x   4096  | type F16  | T+  93\n",
      "[253/291] Writing tensor blk.27.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  93\n",
      "[254/291] Writing tensor blk.27.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  93\n",
      "[255/291] Writing tensor blk.28.attn_norm.weight                | size   4096           | type F32  | T+  93\n",
      "[256/291] Writing tensor blk.28.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  97\n",
      "[257/291] Writing tensor blk.28.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  97\n",
      "[258/291] Writing tensor blk.28.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  97\n",
      "[259/291] Writing tensor blk.28.ffn_norm.weight                 | size   4096           | type F32  | T+  97\n",
      "[260/291] Writing tensor blk.28.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  97\n",
      "[261/291] Writing tensor blk.28.attn_output.weight              | size   4096 x   4096  | type F16  | T+  97\n",
      "[262/291] Writing tensor blk.28.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  97\n",
      "[263/291] Writing tensor blk.28.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  97\n",
      "[264/291] Writing tensor blk.29.attn_norm.weight                | size   4096           | type F32  | T+  97\n",
      "[265/291] Writing tensor blk.29.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 101\n",
      "[266/291] Writing tensor blk.29.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 101\n",
      "[267/291] Writing tensor blk.29.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 101\n",
      "[268/291] Writing tensor blk.29.ffn_norm.weight                 | size   4096           | type F32  | T+ 101\n",
      "[269/291] Writing tensor blk.29.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 101\n",
      "[270/291] Writing tensor blk.29.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 101\n",
      "[271/291] Writing tensor blk.29.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 101\n",
      "[272/291] Writing tensor blk.29.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 101\n",
      "[273/291] Writing tensor blk.30.attn_norm.weight                | size   4096           | type F32  | T+ 101\n",
      "[274/291] Writing tensor blk.30.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 105\n",
      "[275/291] Writing tensor blk.30.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 105\n",
      "[276/291] Writing tensor blk.30.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 105\n",
      "[277/291] Writing tensor blk.30.ffn_norm.weight                 | size   4096           | type F32  | T+ 105\n",
      "[278/291] Writing tensor blk.30.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 105\n",
      "[279/291] Writing tensor blk.30.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 105\n",
      "[280/291] Writing tensor blk.30.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 105\n",
      "[281/291] Writing tensor blk.30.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 105\n",
      "[282/291] Writing tensor blk.31.attn_norm.weight                | size   4096           | type F32  | T+ 105\n",
      "[283/291] Writing tensor blk.31.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 108\n",
      "[284/291] Writing tensor blk.31.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 108\n",
      "[285/291] Writing tensor blk.31.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 109\n",
      "[286/291] Writing tensor blk.31.ffn_norm.weight                 | size   4096           | type F32  | T+ 109\n",
      "[287/291] Writing tensor blk.31.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 109\n",
      "[288/291] Writing tensor blk.31.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 109\n",
      "[289/291] Writing tensor blk.31.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 109\n",
      "[290/291] Writing tensor blk.31.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 109\n",
      "[291/291] Writing tensor output_norm.weight                     | size   4096           | type F32  | T+ 109\n",
      "Wrote merged_model/ggml-model-f16.gguf\n"
     ]
    }
   ],
   "source": [
    "# Convert the 7B model to ggml FP16 format\n",
    "!python3 convert.py merged_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55eecd07-4900-4634-8a1b-77084f37067c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # [Optional] for models using BPE tokenizers\n",
    "# !python convert.py model --vocabtype bpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e3b4fcb-4829-4215-9daf-77daa2846ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: ./quantize: not found\n"
     ]
    }
   ],
   "source": [
    "# # [Optional] quantize the model to 4-bits (using q4_0 method)\n",
    "# Will need llama.cpp installed!\n",
    "#!./quantize ./merged_model/ggml-model-f16.gguf ./merged_model/ggml-model-q4_0.gguf q4_0\n",
    "\n",
    "!docker run --gpus all -v .:/models -it ghcr.io/ggerganov/llama.cpp:full-cuda --quantize /models/merged_model/ggml-model-f16.gguf /models/merged_model/ggml-model-q4_0.gguf q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0141d63-871f-448f-92ae-ab631fc38090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # [Optional] update the gguf filetype to current if older version is unsupported by another application\n",
    "# !./quantize ./model/ggml-model-q4_0.gguf ./model/ggml-model-q4_0-v2.gguf COPY"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
